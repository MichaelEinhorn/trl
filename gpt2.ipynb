{"cells":[{"cell_type":"markdown","metadata":{"id":"uPUzqhylyE5n"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MarcCote/TextWorld/blob/msr_summit_2021/notebooks/Building%20a%20simple%20agent.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"BJMSoUNwyE5r"},"source":["# Building a simple agent with TextWorld\n","\n","This tutorial outlines the steps to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/My Drive\n","%cd trl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqhSsad9Ekik","executionInfo":{"status":"ok","timestamp":1661204255945,"user_tz":420,"elapsed":2677,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"699529f6-2444-4ac3-a770-158ee6cc785d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive\n","/content/drive/My Drive/trl\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"mnNgYvt6v0JI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"icZs6NL4yE5s"},"source":["### Prerequisite\n","Install TextWorld as described in the [README.md](https://github.com/microsoft/TextWorld#readme). Most of the time, a simple `pip install` should work."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWTvsDGGyE5t","executionInfo":{"status":"ok","timestamp":1661204259434,"user_tz":420,"elapsed":3494,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"5753a9db-4b1c-4c35-c667-54ccd066d845"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: textworld in /usr/local/lib/python3.7/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from textworld) (1.21.6)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textworld) (8.14.0)\n","Requirement already satisfied: mementos>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from textworld) (1.3.1)\n","Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from textworld) (1.15.1)\n","Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.7/dist-packages (from textworld) (2.0.10)\n","Requirement already satisfied: tatsu>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from textworld) (4.4.0)\n","Requirement already satisfied: tqdm>=4.17.1 in /usr/local/lib/python3.7/dist-packages (from textworld) (4.64.0)\n","Requirement already satisfied: jericho>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from textworld) (3.1.0)\n","Requirement already satisfied: hashids>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from textworld) (1.3.1)\n","Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from textworld) (2.6.3)\n","Requirement already satisfied: gym>=0.10.11 in /usr/local/lib/python3.7/dist-packages (from textworld) (0.25.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->textworld) (2.21)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.11->textworld) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.11->textworld) (1.5.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.11->textworld) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.10.11->textworld) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.10.11->textworld) (4.1.1)\n","Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from jericho>=3.0.3->textworld) (3.4.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.10)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.0.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (21.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (57.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.23.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.11.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.6.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (2.4.4)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (1.9.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (3.3.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (0.4.2)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->jericho>=3.0.3->textworld) (8.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.1.0->jericho>=3.0.3->textworld) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (1.24.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=2.1.0->jericho>=3.0.3->textworld) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.1.0->jericho>=3.0.3->textworld) (2.0.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit->textworld) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit->textworld) (0.2.5)\n"]}],"source":["!pip install textworld"]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6ZX-nVT_f3o","executionInfo":{"status":"ok","timestamp":1661204265856,"user_tz":420,"elapsed":6431,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"6e72a3ad-7a4e-4588-d410-1502592039ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"tTJ5BPyMyE5v"},"source":["and [PyTorch](https://pytorch.org/) (tested with both v1.8.2 and v.1.9.0)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-hFU5D6yE5w","executionInfo":{"status":"ok","timestamp":1661204266461,"user_tz":420,"elapsed":614,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"8b7dd308-b64c-43a1-ce0e-5fc03ad109b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n"]}],"source":["IS_COLAB = False\n","try:\n","    import google.colab, torch, sys\n","    print(torch.__version__)\n","    if not torch.cuda.is_available():\n","        print(\"Change runtime type to include a GPU.\")\n","    IS_COLAB = True\n","except:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"8d_K9f1byE5x"},"source":["**[Optional]** Download all data beforehand. Otherwise, they are going to be generated as needed (slower)."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiQnT0WayE5z","executionInfo":{"status":"ok","timestamp":1661204276740,"user_tz":420,"elapsed":10286,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"b546ab23-7b14-47d4-b56d-e486eff4b19b"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-08-22 21:37:46--  https://aka.ms/textworld/notebooks/data.zip\n","Resolving aka.ms (aka.ms)... 23.0.129.113\n","Connecting to aka.ms (aka.ms)|23.0.129.113|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://textworld.blob.core.windows.net/$web/notebooks/data.zip [following]\n","--2022-08-22 21:37:46--  https://textworld.blob.core.windows.net/$web/notebooks/data.zip\n","Resolving textworld.blob.core.windows.net (textworld.blob.core.windows.net)... 52.239.172.164\n","Connecting to textworld.blob.core.windows.net (textworld.blob.core.windows.net)|52.239.172.164|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 26926729 (26M) [application/octet-stream]\n","Saving to: ‘data.zip’\n","\n","data.zip            100%[===================>]  25.68M  4.13MB/s    in 8.5s    \n","\n","2022-08-22 21:37:56 (3.01 MB/s) - ‘data.zip’ saved [26926729/26926729]\n","\n"]}],"source":["!wget https://aka.ms/textworld/notebooks/data.zip\n","!unzip -nq data.zip && rm -f data.zip"]},{"cell_type":"markdown","metadata":{"id":"is8jBmJ4yE50"},"source":["## Learning challenges\n","Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n","\n","1. a combinatorial action space (that grows w.r.t. vocabulary)\n","2. a really sparse reward signal.\n","\n","To ease the learning process, we will be requesting additional information alongside the game's narrative (as covered in [Playing TextWorld generated games with OpenAI Gym](Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb#Interact-with-the-game)). More specifically, we will request the following information:\n","\n","- __Description__:\n","For every game state, we will get the output of the `look` command which describes the current location;\n","\n","- __Inventory__:\n","For every game state, we will get the output of the `inventory` command which describes the player's inventory;\n","\n","- __Admissible commands__:\n","For every game state, we will get the list of commands guaranteed to be understood by the game interpreter;\n","\n","- __Intermediate reward__:\n","For every game state, we will get an intermediate reward which can either be:\n","  - __-1__: last action needs to be undone before resuming the quest\n","  -  __0__: last action didn't affect the quest\n","  -  __1__: last action brought us closer to completing the quest\n","\n","- __Entities__:\n","For every game, we will get a list of entity names that the agent can interact with.\n"]},{"cell_type":"markdown","metadata":{"id":"V3GN-suFyE53"},"source":["## Simple test games\n","We can use TextWorld to generate a few simple games with the following handcrafted world\n","```\n","                     Bathroom\n","                        +\n","                        |\n","                        +\n","    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n","      (P)               +                  +\n","                        |                  |\n","                        +                  +\n","                   Living Room           Garden\n","```\n","where the goal is always to retrieve a hidden food item and put it on the stove which is located in the kitchen. One can lose the game if it eats the food item instead of putting it on the stove!\n","\n","Using `tw-make tw-simple ...`, we are going to generate the following 7 games:\n","\n","| gamefile | description |\n","| :------- | :---------- |\n","| `games/rewardsDense_goalDetailed.z8` | dense reward + detailed instructions |\n","| `games/rewardsBalanced_goalDetailed.z8` | balanced rewards + detailed instructions |\n","| `games/rewardsSparse_goalDetailed.z8` | sparse rewards + detailed instructions |\n","| | |\n","| `games/rewardsDense_goalBrief.z8` | dense rewards + no instructions but the goal is mentionned |\n","| `games/rewardsBalanced_goalBrief.z8` | balanced rewards + no instructions but the goal is mentionned |\n","| `games/rewardsSparse_goalBrief.z8` | sparse rewards + no instructions but the goal is mentionned |\n","| | |\n","| `games/rewardsSparse_goalNone.z8` | sparse rewards + no instructions/goal<br>_Hint: there's an hidden note in the game that describes the goal!_ |"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2fXqhZzMyE55","executionInfo":{"status":"ok","timestamp":1661204295821,"user_tz":420,"elapsed":359,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}}},"outputs":[],"source":["# You can skip this if you already downloaded the games in the prequisite section.\n","if False:\n","  # Same as !make_games.sh\n","  !tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8\n","  !tw-make tw-simple --rewards balanced --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalDetailed.z8\n","  !tw-make tw-simple --rewards sparse   --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalDetailed.z8\n","  !tw-make tw-simple --rewards dense    --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsDense_goalBrief.z8\n","  !tw-make tw-simple --rewards balanced --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalBrief.z8\n","  !tw-make tw-simple --rewards sparse   --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8\n","  !tw-make tw-simple --rewards sparse   --goal none     --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8"]},{"cell_type":"markdown","metadata":{"id":"vCFwvj-_yE57"},"source":["## Building the random baseline\n","Let's start with building an agent that simply selects an admissible command at random."]},{"cell_type":"code","source":[""],"metadata":{"id":"Z0LWqo9p4zjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"IubeCZ8AyE57","executionInfo":{"status":"ok","timestamp":1661204299711,"user_tz":420,"elapsed":954,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}}},"outputs":[],"source":["from typing import Mapping, Any\n","\n","import numpy as np\n","\n","import textworld.gym\n","\n","\n","class RandomAgent(textworld.gym.Agent):\n","    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n","    def __init__(self, seed=1234):\n","        self.seed = seed\n","        self.rng = np.random.RandomState(self.seed)\n","\n","    @property\n","    def infos_to_request(self) -> textworld.EnvInfos:\n","        return textworld.EnvInfos(admissible_commands=True)\n","    \n","    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n","        return self.rng.choice(infos[\"admissible_commands\"])\n"]},{"cell_type":"code","source":["class HumanAgent(textworld.gym.Agent):\n","    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n","    def __init__(self, seed=1234):\n","        self.seed = seed\n","        self.rng = np.random.RandomState(self.seed)\n","\n","    @property\n","    def infos_to_request(self) -> textworld.EnvInfos:\n","        return textworld.EnvInfos(admissible_commands=True)\n","    \n","    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n","        print(obs)\n","        return input()"],"metadata":{"id":"mU6zr6hkApHX","executionInfo":{"status":"ok","timestamp":1661204302353,"user_tz":420,"elapsed":2,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HvWUj1myE59"},"source":["## Play function\n","Let's write a simple play function that we will use to evaluate our agent on a given game."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MXggmJpDyE59","executionInfo":{"status":"ok","timestamp":1661204304780,"user_tz":420,"elapsed":457,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}}},"outputs":[],"source":["import os\n","from glob import glob\n","\n","import gym\n","import textworld.gym\n","\n","import torch\n","\n","\n","def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n","    torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n","\n","    infos_to_request = agent.infos_to_request\n","    infos_to_request.max_score = True  # Needed to normalize the scores.\n","    \n","    gamefiles = [path]\n","    if os.path.isdir(path):\n","        gamefiles = glob(os.path.join(path, \"*.z8\"))\n","        \n","    print(gamefiles)\n","    env_id = textworld.gym.register_games(gamefiles,\n","                                          request_infos=infos_to_request,\n","                                          max_episode_steps=max_step)\n","    print(env_id)\n","    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n","    if verbose:\n","        if os.path.isdir(path):\n","            print(os.path.dirname(path), end=\"\")\n","        else:\n","            print(os.path.basename(path), end=\"\")\n","        \n","    # Collect some statistics: nb_steps, final reward.\n","    avg_moves, avg_scores, avg_norm_scores = [], [], []\n","    for no_episode in range(nb_episodes):\n","        obs, infos = env.reset()  # Start new episode.\n","\n","        score = 0\n","        done = False\n","        nb_moves = 0\n","        while not done:\n","            command = agent.act(obs, score, done, infos)\n","            obs, score, done, infos = env.step(command)\n","            nb_moves += 1\n","        \n","        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n","                \n","        if verbose:\n","            print(\".\", end=\"\")\n","        avg_moves.append(nb_moves)\n","        avg_scores.append(score)\n","        avg_norm_scores.append(score / infos[\"max_score\"])\n","\n","    env.close()\n","    if verbose:\n","        if os.path.isdir(path):\n","            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n","            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n","        else:\n","            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n","            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n","    "]},{"cell_type":"markdown","metadata":{"id":"KgZp0TyOyE5-"},"source":["#### Evaluate the random agent"]},{"cell_type":"code","source":["%pwd\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ID0GlEDwd6V-","executionInfo":{"status":"ok","timestamp":1661204309531,"user_tz":420,"elapsed":1071,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"79d46614-d0b2-492a-8ace-85a1e9041df0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/     LICENSE      README.md         \u001b[01;34mtesting_games\u001b[0m/      \u001b[01;34mtrl\u001b[0m/\n","CONTRIBUTING.md  Makefile     requirements.txt  test_trl_gptneo.py\n","\u001b[01;34mdocs\u001b[0m/            MANIFEST.in  settings.ini      test_trl.py\n","\u001b[01;34mgames\u001b[0m/           \u001b[01;34mnbs\u001b[0m/         setup.py          \u001b[01;34mtraining_games\u001b[0m/\n"]}]},{"cell_type":"code","source":["import os\n","print(os.path.exists(\"./games/tw-rewardsDense_goalDetailed.z8\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqbOAitveJzD","executionInfo":{"status":"ok","timestamp":1661204311811,"user_tz":420,"elapsed":330,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"975bb47e-18d7-401f-e72f-7560fb7cb5d8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"c7eTIGDHyE5-","executionInfo":{"status":"error","timestamp":1661204314795,"user_tz":420,"elapsed":412,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"f62468ae-e9f7-4709-e786-f809bd9cfffa"},"outputs":[{"output_type":"stream","name":"stdout","text":["['./games/tw-rewardsDense_goalDetailed.z8']\n","tw-v0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:441: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  \"The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\"\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ff130fdd2b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We report the score and steps averaged over 10 playthroughs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsDense_goalDetailed.z8\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Dense rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsBalanced_goalDetailed.z8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Balanced rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsSparse_goalDetailed.z8\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Sparse rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-b12ad749eb1e>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           max_episode_steps=max_step)\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a Gym environment to play the text game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mdisable_env_checker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_env_checker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     ):\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPassiveEnvChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepAPICompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_step_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action_space\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         ), \"The environment must specify an action space. https://www.gymlibrary.ml/content/environment_creation/\"\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcheck_action_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         assert hasattr(\n\u001b[1;32m     25\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"observation_space\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36mcheck_space\u001b[0;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         raise AssertionError(\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;34mf\"{space_type} space does not inherit from `gym.spaces.Space`, actual type: {type(space)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: action space does not inherit from `gym.spaces.Space`, actual type: <class 'NoneType'>"]}],"source":["# We report the score and steps averaged over 10 playthroughs.\n","play(RandomAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")    # Dense rewards\n","play(RandomAgent(), \"./games/tw-rewardsBalanced_goalDetailed.z8\") # Balanced rewards\n","play(RandomAgent(), \"./games/tw-rewardsSparse_goalDetailed.z8\")   # Sparse rewards"]},{"cell_type":"code","source":["play(HumanAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":800},"id":"hwOUgTGoAm4u","executionInfo":{"status":"error","timestamp":1661204187375,"user_tz":420,"elapsed":351,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"6a726c0b-431e-4cbd-e78e-439e0d8c8b74"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["['./games/tw-rewardsDense_goalDetailed.z8']\n","tw-v6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:441: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  \"The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\"\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-cf571ccc41b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHumanAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsDense_goalDetailed.z8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-b12ad749eb1e>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           max_episode_steps=max_step)\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a Gym environment to play the text game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mdisable_env_checker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_env_checker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     ):\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPassiveEnvChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepAPICompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_step_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action_space\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         ), \"The environment must specify an action space. https://www.gymlibrary.ml/content/environment_creation/\"\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcheck_action_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         assert hasattr(\n\u001b[1;32m     25\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"observation_space\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36mcheck_space\u001b[0;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         raise AssertionError(\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;34mf\"{space_type} space does not inherit from `gym.spaces.Space`, actual type: {type(space)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: action space does not inherit from `gym.spaces.Space`, actual type: <class 'NoneType'>"]}]},{"cell_type":"markdown","metadata":{"id":"7QR5htLYyE6A"},"source":["## Neural agent\n","\n","Now, let's create an agent that can learn to play text-based games. The agent will be trained to select a command from the list of admissible commands given the current game's narrative, inventory, and room description. Here is an overview of the architecture used for the agent: \n","\n","<div>\n","  <img src=\"https://raw.githubusercontent.com/MarcCote/TextWorld/msr_summit_2021/notebooks/figs/neural_agent.png\" width=\"500\"/>\n","</div>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rWMuW7wKyE6A"},"source":["### Code\n","Here's the implementation of that learning agent built with [PyTorch](https://pytorch.org/)."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuwfllu5yE6B","executionInfo":{"status":"ok","timestamp":1661203662391,"user_tz":420,"elapsed":9,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"df1ebc49-3fa5-4b86-c3fd-1cc82f872133"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"stream","name":"stderr","text":["<>:118: DeprecationWarning: invalid escape sequence \\-\n"]}],"source":["import re\n","from typing import List, Mapping, Any, Optional\n","from collections import defaultdict\n","\n","import numpy as np\n","\n","import textworld\n","import textworld.gym\n","from textworld import EnvInfos\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","\n","class CommandScorer(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(CommandScorer, self).__init__()\n","        torch.manual_seed(42)  # For reproducibility\n","        self.embedding    = nn.Embedding(input_size, hidden_size)\n","        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n","        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n","        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n","        self.hidden_size  = hidden_size\n","        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n","        self.critic       = nn.Linear(hidden_size, 1)\n","        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n","\n","    def forward(self, obs, commands, **kwargs):\n","        input_length = obs.size(0)\n","        batch_size = obs.size(1)\n","        nb_cmds = commands.size(1)\n","\n","        embedded = self.embedding(obs)\n","        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n","        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n","        self.state_hidden = state_hidden\n","        value = self.critic(state_output)\n","\n","        # Attention network over the commands.\n","        cmds_embedding = self.embedding.forward(commands)\n","        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n","\n","        # Same observed state for all commands.\n","        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n","\n","        # Same command choices for the whole batch.\n","        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n","\n","        # Concatenate the observed state and command encodings.\n","        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n","\n","        # Compute one score per command.\n","        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n","\n","        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n","        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n","        return scores, index, value\n","\n","    def reset_hidden(self, batch_size):\n","        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n","\n","\n","class NeuralAgent:\n","    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n","    MAX_VOCAB_SIZE = 1000\n","    UPDATE_FREQUENCY = 10\n","    LOG_FREQUENCY = 1000\n","    GAMMA = 0.9\n","    \n","    def __init__(self) -> None:\n","        self._initialized = False\n","        self._epsiode_has_started = False\n","        self.id2word = [\"<PAD>\", \"<UNK>\"]\n","        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n","        \n","        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n","        if device == \"cuda\":\n","          self.model = self.model.cuda()\n","          \n","        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n","        \n","        self.mode = \"test\"\n","    \n","    def train(self):\n","        self.mode = \"train\"\n","        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n","        self.transitions = []\n","        self.model.reset_hidden(1)\n","        self.last_score = 0\n","        self.no_train_step = 0\n","    \n","    def test(self):\n","        self.mode = \"test\"\n","        self.model.reset_hidden(1)\n","        \n","    @property\n","    def infos_to_request(self) -> EnvInfos:\n","        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n","                        won=True, lost=True)\n","    \n","    def _get_word_id(self, word):\n","        if word not in self.word2id:\n","            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n","                return self.word2id[\"<UNK>\"]\n","            \n","            self.id2word.append(word)\n","            self.word2id[word] = len(self.word2id)\n","            \n","        return self.word2id[word]\n","            \n","    def _tokenize(self, text):\n","        # Simple tokenizer: strip out all non-alphabetic characters.\n","        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n","        word_ids = list(map(self._get_word_id, text.split()))\n","        return word_ids\n","\n","    def _process(self, texts):\n","        texts = list(map(self._tokenize, texts))\n","        max_len = max(len(l) for l in texts)\n","        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n","\n","        for i, text in enumerate(texts):\n","            padded[i, :len(text)] = text\n","\n","        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n","        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n","        return padded_tensor\n","      \n","    def _discount_rewards(self, last_values):\n","        returns, advantages = [], []\n","        R = last_values.data\n","        for t in reversed(range(len(self.transitions))):\n","            rewards, _, _, values = self.transitions[t]\n","            R = rewards + self.GAMMA * R\n","            adv = R - values\n","            returns.append(R)\n","            advantages.append(adv)\n","            \n","        return returns[::-1], advantages[::-1]\n","\n","    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n","        \n","        # Build agent's observation: feedback + look + inventory.\n","        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n","        \n","        # Tokenize and pad the input and the commands to chose from.\n","        input_tensor = self._process([input_]).to(device)\n","        commands_tensor = self._process(infos[\"admissible_commands\"]).to(device)\n","        \n","        # Get our next action and value prediction.\n","        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n","        action = infos[\"admissible_commands\"][indexes[0]]\n","        \n","        if self.mode == \"test\":\n","            if done:\n","                self.model.reset_hidden(1)\n","            return action\n","        \n","        self.no_train_step += 1\n","        \n","        if self.transitions:\n","            reward = score - self.last_score  # Reward is the gain/loss in score.\n","            self.last_score = score\n","            if infos[\"won\"]:\n","                reward += 100\n","            if infos[\"lost\"]:\n","                reward -= 100\n","                \n","            self.transitions[-1][0] = reward  # Update reward information.\n","        \n","        self.stats[\"max\"][\"score\"].append(score)\n","        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n","            # Update model\n","            returns, advantages = self._discount_rewards(values)\n","            \n","            loss = 0\n","            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n","                reward, indexes_, outputs_, values_ = transition\n","                \n","                advantage        = advantage.detach() # Block gradients flow here.\n","                probs            = F.softmax(outputs_, dim=2)\n","                log_probs        = torch.log(probs)\n","                log_action_probs = log_probs.gather(2, indexes_)\n","                policy_loss      = (-log_action_probs * advantage).sum()\n","                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n","                entropy     = (-probs * log_probs).sum()\n","                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n","                \n","                self.stats[\"mean\"][\"reward\"].append(reward)\n","                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n","                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n","                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n","                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n","            \n","            if self.no_train_step % self.LOG_FREQUENCY == 0:\n","                msg = \"{:6d}. \".format(self.no_train_step)\n","                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n","                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n","                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n","                print(msg)\n","                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n","            \n","            loss.backward()\n","            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","        \n","            self.transitions = []\n","            self.model.reset_hidden(1)\n","        else:\n","            # Keep information about transitions for Truncated Backpropagation Through Time.\n","            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n","        \n","        if done:\n","            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n","        \n","        return action"]},{"cell_type":"code","source":["from collections import deque\n","\n","class buffer:\n","    def __init__(self, max_size):\n","      self.max_size = max_size\n","      self.queue = deque([])\n","\n","    def __len__(self):\n","      return len(self.queue)\n","\n","    def __iter__(self):\n","      return iter(self.queue)\n","\n","    def append(self, obj):\n","      self.queue.append(obj)\n","      if len(self.queue) > self.max_size:\n","        return self.queue.pop()\n","      else:\n","        return None\n","\n","    def clear(self):\n","      self.queue = deque([])\n","\n","# Utilities for dealing with tokens\n","def make_inputs(tokenizer, prompts, device=\"cuda\"):\n","    token_lists = [tokenizer.encode(p) for p in prompts]\n","    maxlen = max(len(t) for t in token_lists)\n","    if \"[PAD]\" in tokenizer.all_special_tokens:\n","        pad_id = tokenizer.all_special_ids[tokenizer.all_special_tokens.index(\"[PAD]\")]\n","    else:\n","        pad_id = 0\n","    input_ids = [[pad_id] * (maxlen - len(t)) + t for t in token_lists]\n","    # position_ids = [[0] * (maxlen - len(t)) + list(range(len(t))) for t in token_lists]\n","    attention_mask = [[0] * (maxlen - len(t)) + [1] * len(t) for t in token_lists]\n","    return dict(\n","        input_ids=torch.tensor(input_ids).to(device),\n","    #    position_ids=torch.tensor(position_ids).to(device),\n","        attention_mask=torch.tensor(attention_mask).to(device),\n","    )\n","\n","def predict_token(mt, prompts, return_p=False):\n","    inp = make_inputs(mt.tokenizer, prompts)\n","    preds, p = predict_from_input(mt.model, inp)\n","    result = [mt.tokenizer.decode(c) for c in preds]\n","    if return_p:\n","        result = (result, p)\n","    return result\n","\n","\n","def predict_from_input(model, inp):\n","    out = model(**inp)[\"logits\"]\n","    probs = torch.softmax(out[:, -1], dim=1)\n","    p, preds = torch.max(probs, dim=1)\n","    return preds, p"],"metadata":{"id":"Jb5D6J67xwu_","executionInfo":{"status":"ok","timestamp":1661203662392,"user_tz":420,"elapsed":6,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from transformers import top_k_top_p_filtering\n","from torch.nn import Identity\n","import torch.nn.functional as F\n","\n","class NLPAgent:\n","    \"\"\" Hugging Face Transformer Agent \"\"\"\n","    UPDATE_FREQUENCY = 10\n","    LOG_FREQUENCY = 1000\n","    GAMMA = 0.9\n","    MEMORY_LEN = 3\n","    \n","    def __init__(self, model, model_ref, tokenizer, humanTurns=0) -> None:\n","        self._initialized = False\n","        self._epsiode_has_started = False\n","       \n","        self.memory = buffer(self.MEMORY_LEN)\n","\n","        self.model = model\n","        self.tokenizer = tokenizer\n","\n","        self.humanTurns = humanTurns\n","        self.humanTurnsRem = self.humanTurns\n","        \n","        if model_ref is not None:\n","          # initialize trainer\n","          ppo_config = {'batch_size': 1, 'forward_batch_size': 1}\n","          self.ppo_trainer = PPOTrainer(model, model_ref, tokenizer, **ppo_config)\n","          self.valueHead = self.ppo_trainer.valueHead\n","\n","        if device == \"cuda\":\n","          self.model.cuda()\n","          self.tokenizer.cuda()\n","          \n","        self.mode = \"test\"\n","\n","        self.clearTextWorldArt = True\n","    \n","    def train(self):\n","        if self.model_ref is None:\n","          raise NotImplementedError\n","        self.mode = \"train\"\n","        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n","        self.transitions = []\n","        # self.model.reset_hidden(1)\n","        self.last_score = 0\n","        self.no_train_step = 0\n","    \n","    def test(self):\n","        self.mode = \"test\"\n","        # self.model.reset_hidden(1)\n","        \n","    @property\n","    def infos_to_request(self) -> EnvInfos:\n","        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n","                        won=True, lost=True)\n","      \n","    def _discount_rewards(self, last_values):\n","        returns, advantages = [], []\n","        R = last_values.data\n","        for t in reversed(range(len(self.transitions))):\n","            rewards, _, _, values = self.transitions[t]\n","            R = rewards + self.GAMMA * R\n","            adv = R - values\n","            returns.append(R)\n","            advantages.append(adv)\n","            \n","        return returns[::-1], advantages[::-1]\n","\n","    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n","        if self.clearTextWorldArt:\n","          self.clearTextWorldArt = False\n","          obs = obs[obs.index(\"Welcome to TextWorld!\"):]\n","        \n","        # Build agent's observation: feedback + look + inventory.\n","        pastStates = \"\"\n","        for mem in self.memory:\n","          pastStates = pastStates + mem + \"\\n\"\n","        input_ = \"{}\\n{}\\n{}\\n{}\\n> \".format(obs, infos[\"description\"], infos[\"inventory\"], infos[\"admissible_commands\"])\n","        prompt = pastStates + input_\n","        \n","        # grabs value of last token in action\n","        values = 0\n","        # convert text to tensor\n","        input_ids = self.tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n","        print(\"prompt tokens: \", input_ids.shape)\n","        print(input_)\n","\n","        if self.humanTurnsRem > 0:\n","          action = input()\n","          self.memory.append(input_ + action)\n","          self.humanTurnsRem -= 1\n","          return action\n","\n","        new_tokens = 0\n","        next_token = None\n","        \n","        while new_tokens == 0 or (new_tokens < 20 and \"\\n\" not in self.tokenizer.decode(next_token) \n","                                  and next_token != self.tokenizer.eos_token):\n","          # run model\n","          with torch.no_grad():\n","                # get logits, only get last value\n","                logits, hidden_state = self.model(input_ids, output_hidden_states=True)\n","                \n","                hidden_state = hidden_state[-1]\n","                values = self.valueHead(hidden_state)\n","                \n","                next_token_logits = logits[:, -1, :]\n","                next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=0, top_p=1)\n","                probs = F.softmax(next_token_logits, dim=-1)\n","                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n","                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n","                \n","                new_tokens += 1\n","        \n","        action_tens = input_ids[:,-new_tokens:]\n","        prompt_tens = input_ids[:,:-new_tokens]\n","\n","        action = self.tokenizer.decode(action_tens[0,:])\n","\n","        print(\"action\")\n","        print(action)\n","\n","        self.memory.append(input_ + action)\n","        \n","        if self.mode == \"test\":\n","            # if done:\n","            #     self.model.reset_hidden(1)\n","            return action\n","        \n","        self.no_train_step += 1\n","        \n","        if self.transitions:\n","            reward = score - self.last_score  # Reward is the gain/loss in score.\n","            self.last_score = score\n","            if infos[\"won\"]:\n","                reward += 100\n","            if infos[\"lost\"]:\n","                reward -= 100\n","                \n","            self.transitions[-1][0] = reward  # Update reward information. Was initialized as none\n","        \n","        self.stats[\"max\"][\"score\"].append(score)\n","        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n","            # get discounted returns and advantages across multiple actions. Currently not used\n","            returns, advantages = self._discount_rewards(values)\n","\n","            query = []\n","            response = []\n","            rewardList = []\n","\n","            for t in reversed(range(len(self.transitions))):\n","              rewards, prompt, action, values = self.transitions[t]\n","\n","              query.append(prompt[0])\n","              response.append(action[0])\n","              rewardList.append(rewards)\n","\n","            train_stats = self.ppo_trainer.step(query, response, rewardList)\n","\n","            if self.no_train_step % self.LOG_FREQUENCY == 0:\n","              print(train_stats)\n","            \n","            \n","        else:\n","            # Keep information about transitions for ppo\n","            \n","            self.transitions.append([None, prompt_tens, action_tens, values])  # Reward will be set on the next call\n","        \n","        if done:\n","            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n","            self.memory.clear()\n","            self.clearTextWorldArt = True\n","            self.humanTurnsRem = self.humanTurns\n","        \n","        return action"],"metadata":{"id":"98LDtQg6CjNO","executionInfo":{"status":"ok","timestamp":1661203663033,"user_tz":420,"elapsed":647,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["if IS_COLAB:\n","  !pip install accelerate"],"metadata":{"id":"3PQ5YwDMI0Vi","executionInfo":{"status":"ok","timestamp":1661203668768,"user_tz":420,"elapsed":2840,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"110d4e2e-5014-4067-a019-25f23fd70bd8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.12.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.12.1+cu113)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (21.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.8)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.1.1)\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlEgcVAuIQWo","executionInfo":{"status":"ok","timestamp":1661203672647,"user_tz":420,"elapsed":3883,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"9be1d2f6-2de4-4d00-ed5d-7ced9aa2563e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"]}]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2Model\n","# from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n","# from trl.ppo import PPOTrainer\n","import trl\n","\n","import importlib\n","importlib.reload(trl)\n","from trl.ppoValHead import PPOTrainer\n","\n","from torchinfo import summary\n","\n","# get models\n","# gpt2_model = GPT2HeadWithValueModel.from_pretrained('gpt2')\n","# gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained('gpt2')\n","gpt2_model = GPT2Model.from_pretrained('gpt2')\n","gpt2_model_ref = GPT2Model.from_pretrained('gpt2')\n","gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","summary(gpt2_model)\n","\n","agent = NLPAgent(gpt2_model, gpt2_model_ref, gpt2_tokenizer, humanTurns=0)\n","print(agent.ppo_trainer.valueHead)\n","\n","# model_name = r\"gpt2-xl\"\n","\n","# Note that if you trace other models, you should set noise_level appropriately.\n","# (We use 0.03 for gpt-neox-20b and 0.025 for gpt-j-6b)\n","# model_name = r\"EleutherAI/gpt-neox-20b\"\n","# model_name = r\"EleutherAI/gpt-j-6B\"\n","\n","# torch_dtype = torch.float16 if '20b' in model_name else None\n","\n","# mt = ModelAndTokenizer(model_name, low_cpu_mem_usage=IS_COLAB, torch_dtype=torch_dtype)\n","# predict_token(mt, ['Megan Rapinoe plays the sport of',\n","#                    'The Space Needle is in the city of'\n","#                   ], return_p=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hnyEtRaDJqg","executionInfo":{"status":"ok","timestamp":1661203688056,"user_tz":420,"elapsed":15426,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"8120b492-b50a-4a5b-d25a-204cd0fc2e22"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["ValueHead(\n","  (summary): Linear(in_features=768, out_features=1, bias=True)\n","  (activation): Identity()\n","  (first_dropout): Dropout(p=0.1, inplace=False)\n","  (last_dropout): Identity()\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"pkN6y-r2yE6C"},"source":["### Training the neural agent\n","Let's first evaluate the agent before training to get a sense of its initial performance."]},{"cell_type":"code","execution_count":15,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":763},"id":"_HyFeWb8yE6D","executionInfo":{"status":"error","timestamp":1661203726351,"user_tz":420,"elapsed":1316,"user":{"displayName":"Michael Einhorn","userId":"04443615996412342483"}},"outputId":"4389785f-ffe0-40a4-c370-9387517b982b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:441: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  \"The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\"\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-f42ef0b11447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsDense_goalDetailed.z8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-3d99869e2545>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                           \u001b[0mrequest_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfos_to_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                           max_episode_steps=max_step)\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a Gym environment to play the text game.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mdisable_env_checker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_env_checker\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     ):\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPassiveEnvChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepAPICompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_step_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action_space\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         ), \"The environment must specify an action space. https://www.gymlibrary.ml/content/environment_creation/\"\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcheck_action_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         assert hasattr(\n\u001b[1;32m     25\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"observation_space\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36mcheck_space\u001b[0;34m(space, space_type, check_box_space_fn)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         raise AssertionError(\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;34mf\"{space_type} space does not inherit from `gym.spaces.Space`, actual type: {type(space)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: action space does not inherit from `gym.spaces.Space`, actual type: <class 'NoneType'>"]}],"source":["play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")\\"]},{"cell_type":"markdown","metadata":{"id":"POiGO-PmyE6D"},"source":["Unsurprisingly, the result is not much different from what the random agent can achieve since our neural agent is initialized to a random policy.\n","\n","Let's train the agent for a few episodes."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"pzf_H7ThyE6E"},"outputs":[],"source":["# You can skip this if you already downloaded the data in the prequisite section.\n","if False:\n","  from time import time\n","  agent = NeuralAgent()\n","\n","  print(\"Training\")\n","  agent.train()  # Tell the agent it should update its parameters.\n","  starttime = time()\n","  play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\", nb_episodes=500, verbose=False)  # Dense rewards game.\n","\n","  print(\"Trained in {:.2f} secs\".format(time() - starttime))\n","\n","  # Save the trained agent.\n","  import os\n","  os.makedirs('checkpoints', exist_ok=True)\n","  torch.save(agent, 'checkpoints/agent_trained_on_single_game.pt')"]},{"cell_type":"markdown","metadata":{"id":"NdT8TB7ayE6F"},"source":["#### Testing the agent trained on a single game"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElRy94-tyE6F"},"outputs":[],"source":["# We report the score and steps averaged over 10 playthroughs.\n","agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n","agent.test()\n","play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Dense rewards game."]},{"cell_type":"markdown","metadata":{"id":"gAgebJpEyE6F"},"source":["Of course, since we trained on that single simple game, it's not surprinsing the agent can achieve a high score on it. It would be more interesting to evaluate the generalization capability of the agent.\n","\n","To do so, we are going to test the agent on another game drawn from the same game distribution (i.e. same world but the goal is to pick another food item). Let's generate `games/another_game.z8` with the same rewards density (`--rewards dense`) and the same goal description (`--goal detailed`), but using `--seed 1` and without the `--test` flag (to make sure the game is not part of the test set since `games/rewardsDense_goalDetailed.z8` is)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPKY9jkMyE6G"},"outputs":[],"source":["!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/tw-another_game.z8 -v -f"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"tOILdKH_yE6G"},"outputs":[],"source":["# We report the score and steps averaged over 10 playthroughs.\n","play(RandomAgent(), \"./games/tw-another_game.z8\")\n","play(agent, \"./games/tw-another_game.z8\")"]},{"cell_type":"markdown","metadata":{"id":"3mMvSMb-yE6G"},"source":["As we can see the trained agent does a bit better than the random agent. In order to improve the agent's generalization capability, we should train it on many different games drawn from the game distribution.\n","\n","One could use the following command to easily generate 100 training games:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"W1xWlfjdyE6H"},"outputs":[],"source":["# You can skip this if you already downloaded the data in the prequisite section.\n","\n","! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --format z8 --output training_games/ --seed"]},{"cell_type":"markdown","metadata":{"id":"He0cUnpuyE6H"},"source":["Then, we train our agent on that set of training games."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"UVLj-YKMyE6H"},"outputs":[],"source":["# You can skip this if you already downloaded the data in the prequisite section.\n","if False:\n","  from time import time\n","  agent = NeuralAgent()\n","\n","  print(\"Training on 100 games\")\n","  agent.train()  # Tell the agent it should update its parameters.\n","  starttime = time()\n","  play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n","  print(\"Trained in {:.2f} secs\".format(time() - starttime))\n","\n","  # Save the trained agent.\n","  import os\n","  os.makedirs('checkpoints', exist_ok=True)\n","  torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"]},{"cell_type":"markdown","metadata":{"id":"k_iOg8QpyE6H"},"source":["#### Testing the agent trained on 100 games."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDn1WlTIyE6I"},"outputs":[],"source":["agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n","agent.test()\n","play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n","play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."]},{"cell_type":"markdown","metadata":{"id":"CVVsVTyLyE6I"},"source":["Compare it to the agent trained on a single game."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"9UCCW3cvyE6I"},"outputs":[],"source":["agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n","agent.test()\n","play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n","play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."]},{"cell_type":"markdown","metadata":{"id":"B31IqhZAyE6J"},"source":["#### Evaluating the agent on a test distribution\n","We will generate 20 test games and evaluate the agent on them."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"-ZevzwZbyE6J"},"outputs":[],"source":["# You can skip this if you already downloaded the games in the prequisite section.\n","\n","! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --format z8 --output testing_games/ --seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHrQ8zjzyE6J"},"outputs":[],"source":["agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n","agent.test()\n","play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)\n","play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game."]},{"cell_type":"markdown","metadata":{"id":"zsyMnRm4yE6K"},"source":["While not being perfect, the agent manage to score more points on average compared to the random agent."]},{"cell_type":"markdown","metadata":{"id":"UdHfSBZ4yE6K"},"source":["## Next steps\n","\n","Here are a few possible directions one can take to improve the agent's performance.\n","- Adding more training games\n","- Changing the agent architecture\n","- Leveraging already trained word embeddings\n","- Playing more games at once (see [`textworld.gym.make_batch`](https://textworld.readthedocs.io/en/latest/textworld.gym.html#textworld.gym.utils.make_batch))"]},{"cell_type":"markdown","metadata":{"id":"NepDIonbyE6K"},"source":["\n","## Papers about RL applied to text-based games\n","* [Language Understanding for Text-based games using Deep Reinforcement Learning][narasimhan_et_al_2015]\n","* [Learning How Not to Act in Text-based Games][haroush_et_al_2017]\n","* [Deep Reinforcement Learning with a Natural Language Action Space][he_et_al_2015]\n","* [What can you do with a rock? Affordance extraction via word embeddings][fulda_et_al_2017]\n","* [Text-based adventures of the Golovin AI Agent][kostka_et_al_2017]\n","* [Using reinforcement learning to learn how to play text-based games][zelinka_2018]\n","\n","[narasimhan_et_al_2015]: https://arxiv.org/abs/1506.08941\n","[haroush_et_al_2017]: https://openreview.net/pdf?id=B1-tVX1Pz\n","[he_et_al_2015]: https://arxiv.org/abs/1511.04636\n","[fulda_et_al_2017]: https://arxiv.org/abs/1703.03429\n","[kostka_et_al_2017]: https://arxiv.org/abs/1705.05637\n","[zelinka_2018]: https://arxiv.org/abs/1801.01999"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"gpt2.ipynb","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}