{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPUzqhylyE5n"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MarcCote/TextWorld/blob/msr_summit_2021/notebooks/Building%20a%20simple%20agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJMSoUNwyE5r"
   },
   "source": [
    "# Building a simple agent with TextWorld\n",
    "\n",
    "This tutorial outlines the steps to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1661204255945,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "vqhSsad9Ekik",
    "outputId": "699529f6-2444-4ac3-a770-158ee6cc785d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab, torch, sys\n",
    "    print(torch.__version__)\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Change runtime type to include a GPU.\")\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd drive/My Drive\n",
    "    %cd trl\n",
    "    \n",
    "print(IS_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnNgYvt6v0JI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icZs6NL4yE5s"
   },
   "source": [
    "### Prerequisite\n",
    "Install TextWorld as described in the [README.md](https://github.com/microsoft/TextWorld#readme). Most of the time, a simple `pip install` should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3494,
     "status": "ok",
     "timestamp": 1661204259434,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "EWTvsDGGyE5t",
    "outputId": "5753a9db-4b1c-4c35-c667-54ccd066d845"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !pip install textworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1661204265856,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "i6ZX-nVT_f3o",
    "outputId": "6e72a3ad-7a4e-4588-d410-1502592039ca"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTJ5BPyMyE5v"
   },
   "source": [
    "and [PyTorch](https://pytorch.org/) (tested with both v1.8.2 and v.1.9.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1661204266461,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "Z-hFU5D6yE5w",
    "outputId": "8b7dd308-b64c-43a1-ce0e-5fc03ad109b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d_K9f1byE5x"
   },
   "source": [
    "**[Optional]** Download all data beforehand. Otherwise, they are going to be generated as needed (slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/trl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10286,
     "status": "ok",
     "timestamp": 1661204276740,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "MiQnT0WayE5z",
    "outputId": "b546ab23-7b14-47d4-b56d-e486eff4b19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-24 03:29:03--  https://aka.ms/textworld/notebooks/data.zip\n",
      "Resolving aka.ms (aka.ms)... 96.7.86.32\n",
      "Connecting to aka.ms (aka.ms)|96.7.86.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://textworld.blob.core.windows.net/$web/notebooks/data.zip [following]\n",
      "--2022-08-24 03:29:03--  https://textworld.blob.core.windows.net/$web/notebooks/data.zip\n",
      "Resolving textworld.blob.core.windows.net (textworld.blob.core.windows.net)... 52.239.172.164\n",
      "Connecting to textworld.blob.core.windows.net (textworld.blob.core.windows.net)|52.239.172.164|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26926729 (26M) [application/octet-stream]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  25.68M  25.8MB/s    in 1.0s    \n",
      "\n",
      "2022-08-24 03:29:04 (25.8 MB/s) - ‘data.zip’ saved [26926729/26926729]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://aka.ms/textworld/notebooks/data.zip\n",
    "!unzip -nq data.zip && rm -f data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is8jBmJ4yE50"
   },
   "source": [
    "## Learning challenges\n",
    "Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n",
    "\n",
    "1. a combinatorial action space (that grows w.r.t. vocabulary)\n",
    "2. a really sparse reward signal.\n",
    "\n",
    "To ease the learning process, we will be requesting additional information alongside the game's narrative (as covered in [Playing TextWorld generated games with OpenAI Gym](Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb#Interact-with-the-game)). More specifically, we will request the following information:\n",
    "\n",
    "- __Description__:\n",
    "For every game state, we will get the output of the `look` command which describes the current location;\n",
    "\n",
    "- __Inventory__:\n",
    "For every game state, we will get the output of the `inventory` command which describes the player's inventory;\n",
    "\n",
    "- __Admissible commands__:\n",
    "For every game state, we will get the list of commands guaranteed to be understood by the game interpreter;\n",
    "\n",
    "- __Intermediate reward__:\n",
    "For every game state, we will get an intermediate reward which can either be:\n",
    "  - __-1__: last action needs to be undone before resuming the quest\n",
    "  -  __0__: last action didn't affect the quest\n",
    "  -  __1__: last action brought us closer to completing the quest\n",
    "\n",
    "- __Entities__:\n",
    "For every game, we will get a list of entity names that the agent can interact with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3GN-suFyE53"
   },
   "source": [
    "## Simple test games\n",
    "We can use TextWorld to generate a few simple games with the following handcrafted world\n",
    "```\n",
    "                     Bathroom\n",
    "                        +\n",
    "                        |\n",
    "                        +\n",
    "    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n",
    "      (P)               +                  +\n",
    "                        |                  |\n",
    "                        +                  +\n",
    "                   Living Room           Garden\n",
    "```\n",
    "where the goal is always to retrieve a hidden food item and put it on the stove which is located in the kitchen. One can lose the game if it eats the food item instead of putting it on the stove!\n",
    "\n",
    "Using `tw-make tw-simple ...`, we are going to generate the following 7 games:\n",
    "\n",
    "| gamefile | description |\n",
    "| :------- | :---------- |\n",
    "| `games/rewardsDense_goalDetailed.z8` | dense reward + detailed instructions |\n",
    "| `games/rewardsBalanced_goalDetailed.z8` | balanced rewards + detailed instructions |\n",
    "| `games/rewardsSparse_goalDetailed.z8` | sparse rewards + detailed instructions |\n",
    "| | |\n",
    "| `games/rewardsDense_goalBrief.z8` | dense rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsBalanced_goalBrief.z8` | balanced rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsSparse_goalBrief.z8` | sparse rewards + no instructions but the goal is mentionned |\n",
    "| | |\n",
    "| `games/rewardsSparse_goalNone.z8` | sparse rewards + no instructions/goal<br>_Hint: there's an hidden note in the game that describes the goal!_ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1661204295821,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "2fXqhZzMyE55"
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the games in the prequisite section.\n",
    "if False:\n",
    "  # Same as !make_games.sh\n",
    "  !tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8\n",
    "  !tw-make tw-simple --rewards balanced --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalDetailed.z8\n",
    "  !tw-make tw-simple --rewards sparse   --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalDetailed.z8\n",
    "  !tw-make tw-simple --rewards dense    --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsDense_goalBrief.z8\n",
    "  !tw-make tw-simple --rewards balanced --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalBrief.z8\n",
    "  !tw-make tw-simple --rewards sparse   --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8\n",
    "  !tw-make tw-simple --rewards sparse   --goal none     --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCFwvj-_yE57"
   },
   "source": [
    "## Building the random baseline\n",
    "Let's start with building an agent that simply selects an admissible command at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0LWqo9p4zjh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1661204299711,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "IubeCZ8AyE57"
   },
   "outputs": [],
   "source": [
    "from typing import Mapping, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld.gym\n",
    "\n",
    "\n",
    "class RandomAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "    \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        return self.rng.choice(infos[\"admissible_commands\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661204302353,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "mU6zr6hkApHX"
   },
   "outputs": [],
   "source": [
    "class HumanAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "    \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        print(obs)\n",
    "        return input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HvWUj1myE59"
   },
   "source": [
    "## Play function\n",
    "Let's write a simple play function that we will use to evaluate our agent on a given game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1661204304780,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "MXggmJpDyE59"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import gym\n",
    "import textworld.gym\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n",
    "    torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
    "\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
    "        \n",
    "    print(gamefiles)\n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    print(env_id)\n",
    "    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "        \n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            if hasattr(agent, 'reportScore'):\n",
    "                agent.reportScore(score, done, infos)\n",
    "            nb_moves += 1\n",
    "        \n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "                \n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "        else:\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgZp0TyOyE5-"
   },
   "source": [
    "#### Evaluate the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1071,
     "status": "ok",
     "timestamp": 1661204309531,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "ID0GlEDwd6V-",
    "outputId": "79d46614-d0b2-492a-8ace-85a1e9041df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md  \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  requirements.txt    \u001b[01;34mtesting_games\u001b[0m/\n",
      "LICENSE          \u001b[01;34mdocs\u001b[0m/         settings.ini        \u001b[01;34mtraining_games\u001b[0m/\n",
      "MANIFEST.in      \u001b[01;34mgames\u001b[0m/        setup.py            \u001b[01;34mtrl\u001b[0m/\n",
      "Makefile         gpt2.ipynb    test_trl.py\n",
      "README.md        \u001b[01;34mnbs\u001b[0m/          test_trl_gptneo.py\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1661204311811,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "DqbOAitveJzD",
    "outputId": "975bb47e-18d7-401f-e72f-7560fb7cb5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"./games/tw-rewardsDense_goalDetailed.z8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "error",
     "timestamp": 1661204314795,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "c7eTIGDHyE5-",
    "outputId": "f62468ae-e9f7-4709-e786-f809bd9cfffa"
   },
   "outputs": [],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "play(RandomAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")    # Dense rewards\n",
    "play(RandomAgent(), \"./games/tw-rewardsBalanced_goalDetailed.z8\") # Balanced rewards\n",
    "play(RandomAgent(), \"./games/tw-rewardsSparse_goalDetailed.z8\")   # Sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "error",
     "timestamp": 1661204187375,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "hwOUgTGoAm4u",
    "outputId": "6a726c0b-431e-4cbd-e78e-439e0d8c8b74"
   },
   "outputs": [],
   "source": [
    "play(HumanAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QR5htLYyE6A"
   },
   "source": [
    "## Neural agent\n",
    "\n",
    "Now, let's create an agent that can learn to play text-based games. The agent will be trained to select a command from the list of admissible commands given the current game's narrative, inventory, and room description. Here is an overview of the architecture used for the agent: \n",
    "\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/MarcCote/TextWorld/msr_summit_2021/notebooks/figs/neural_agent.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWMuW7wKyE6A"
   },
   "source": [
    "### Code\n",
    "Here's the implementation of that learning agent built with [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661203662391,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "iuwfllu5yE6B",
    "outputId": "df1ebc49-3fa5-4b86-c3fd-1cc82f872133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:118: DeprecationWarning: invalid escape sequence \\-\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CommandScorer, self).__init__()\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic       = nn.Linear(hidden_size, 1)\n",
    "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, obs, commands, **kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "\n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "\n",
    "        # Attention network over the commands.\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
    "\n",
    "        # Same observed state for all commands.\n",
    "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Same command choices for the whole batch.\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Concatenate the observed state and command encodings.\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
    "\n",
    "        # Compute one score per command.\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
    "\n",
    "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
    "        return scores, index, value\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class NeuralAgent:\n",
    "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
    "    MAX_VOCAB_SIZE = 1000\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 1000\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
    "        if device == \"cuda\":\n",
    "          self.model = self.model.cuda()\n",
    "          \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        won=True, lost=True)\n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "            \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    def _process(self, texts):\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text)] = text\n",
    "\n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "      \n",
    "    def _discount_rewards(self, last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
    "        \n",
    "        # Tokenize and pad the input and the commands to chose from.\n",
    "        input_tensor = self._process([input_]).to(device)\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"]).to(device)\n",
    "        \n",
    "        # Get our next action and value prediction.\n",
    "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{:6d}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
    "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
    "        \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1661203662392,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "Jb5D6J67xwu_"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class buffer:\n",
    "    def __init__(self, max_size):\n",
    "      self.max_size = max_size\n",
    "      self.queue = deque([])\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.queue)\n",
    "\n",
    "    def __iter__(self):\n",
    "      return iter(self.queue)\n",
    "\n",
    "    def append(self, obj):\n",
    "      self.queue.append(obj)\n",
    "      if len(self.queue) > self.max_size:\n",
    "        return self.queue.pop()\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    def clear(self):\n",
    "      self.queue = deque([])\n",
    "\n",
    "# Utilities for dealing with tokens\n",
    "def make_inputs(tokenizer, prompts, device=\"cuda\"):\n",
    "    token_lists = [tokenizer.encode(p) for p in prompts]\n",
    "    maxlen = max(len(t) for t in token_lists)\n",
    "    if \"[PAD]\" in tokenizer.all_special_tokens:\n",
    "        pad_id = tokenizer.all_special_ids[tokenizer.all_special_tokens.index(\"[PAD]\")]\n",
    "    else:\n",
    "        pad_id = 0\n",
    "    input_ids = [[pad_id] * (maxlen - len(t)) + t for t in token_lists]\n",
    "    # position_ids = [[0] * (maxlen - len(t)) + list(range(len(t))) for t in token_lists]\n",
    "    attention_mask = [[0] * (maxlen - len(t)) + [1] * len(t) for t in token_lists]\n",
    "    return dict(\n",
    "        input_ids=torch.tensor(input_ids).to(device),\n",
    "    #    position_ids=torch.tensor(position_ids).to(device),\n",
    "        attention_mask=torch.tensor(attention_mask).to(device),\n",
    "    )\n",
    "\n",
    "def predict_token(mt, prompts, return_p=False):\n",
    "    inp = make_inputs(mt.tokenizer, prompts)\n",
    "    preds, p = predict_from_input(mt.model, inp)\n",
    "    result = [mt.tokenizer.decode(c) for c in preds]\n",
    "    if return_p:\n",
    "        result = (result, p)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_from_input(model, inp):\n",
    "    out = model(**inp)[\"logits\"]\n",
    "    probs = torch.softmax(out[:, -1], dim=1)\n",
    "    p, preds = torch.max(probs, dim=1)\n",
    "    return preds, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1661203663033,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "98LDtQg6CjNO"
   },
   "outputs": [],
   "source": [
    "from transformers import top_k_top_p_filtering\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NLPAgent:\n",
    "    \"\"\" Hugging Face Transformer Agent \"\"\"\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 10\n",
    "    GAMMA = 0.9\n",
    "    MEMORY_LEN = 3\n",
    "    \n",
    "    def __init__(self, model, model_ref, tokenizer, humanTurns=0) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "       \n",
    "        self.memory = buffer(self.MEMORY_LEN)\n",
    "\n",
    "        self.model = model\n",
    "        self.model_ref = model_ref\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.humanTurns = humanTurns\n",
    "        self.humanTurnsRem = self.humanTurns\n",
    "        \n",
    "        if model_ref is not None:\n",
    "          # initialize trainer\n",
    "          ppo_config = {'batch_size': self.UPDATE_FREQUENCY, 'forward_batch_size': 1}\n",
    "          self.ppo_trainer = PPOTrainer(self.model, self.model_ref, self.tokenizer, **ppo_config)\n",
    "          self.valueHead = self.ppo_trainer.valueHead\n",
    "\n",
    "        if device == \"cuda\":\n",
    "          self.model.cuda()\n",
    "          self.tokenizer.cuda()\n",
    "          \n",
    "        self.mode = \"test\"\n",
    "\n",
    "        self.clearTextWorldArt = True\n",
    "    \n",
    "    def train(self):\n",
    "        if self.model_ref is None:\n",
    "          raise NotImplementedError\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        # self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "        self.clearTextWorldArt = True\n",
    "        \n",
    "        self.memory.clear()\n",
    "    \n",
    "    def test(self):\n",
    "        self.clearTextWorldArt = True\n",
    "        self.mode = \"test\"\n",
    "        # self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        won=True, lost=True)\n",
    "      \n",
    "    def _discount_rewards(self, last_values=None):\n",
    "        returns, advantages = [], []\n",
    "        if last_values is None:\n",
    "            # not sure if this makes sense for when there is no next state\n",
    "            _, _, _, R = self.transitions[-1]\n",
    "        else:\n",
    "            R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "    \n",
    "    # fill in results from action and train if time\n",
    "    def reportScore(self, score, done, infos):\n",
    "        if self.mode == \"train\":\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "\n",
    "            self.transitions[-1][0] = reward  # Update reward information. Was initialized as none\n",
    "\n",
    "            self.no_train_step += 1\n",
    "\n",
    "            self.stats[\"max\"][\"score\"].append(score)\n",
    "            if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "                # get discounted returns and advantages across multiple actions. Currently not used\n",
    "                returns, advantages = self._discount_rewards()\n",
    "\n",
    "                query = []\n",
    "                response = []\n",
    "                rewardList = []\n",
    "\n",
    "                for t in reversed(range(len(self.transitions))):\n",
    "                  rew, prompt, action, values = self.transitions[t]\n",
    "\n",
    "                  query.append(prompt[0])\n",
    "                  response.append(action[0])\n",
    "                  rewardList.append(rew)\n",
    "\n",
    "                train_stats = self.ppo_trainer.step(query, response, rewardList)\n",
    "\n",
    "                if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                  print(train_stats)\n",
    "\n",
    "                self.transitions = []\n",
    "        \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        if self.clearTextWorldArt:\n",
    "          self.clearTextWorldArt = False\n",
    "          if \"Welcome to TextWorld!\" in obs:\n",
    "              obs = obs[obs.index(\"Welcome to TextWorld!\"):]\n",
    "          elif \"$$$$$$$\" in obs:\n",
    "              obs = obs[obs.rindex(\"$$$$$$$\"):]\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        pastStates = \"\"\n",
    "        for mem in self.memory:\n",
    "          pastStates = pastStates + mem + \"\\n\"\n",
    "        admissible_commands_str = \"options: \"\n",
    "        for adm_cmd in infos[\"admissible_commands\"]:\n",
    "            admissible_commands_str += adm_cmd + \", \"\n",
    "        input_ = \"{}\\n{}\\n{}\\n{}\\nYou\".format(obs, infos[\"description\"], infos[\"inventory\"], admissible_commands_str)\n",
    "        prompt = pastStates + input_\n",
    "        \n",
    "        # grabs value of last token in action\n",
    "        values = 0\n",
    "        # convert text to tensor\n",
    "        input_ids = self.tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        print(\"prompt tokens: \", input_ids.shape)\n",
    "        print(input_)\n",
    "\n",
    "        if self.humanTurnsRem > 0:\n",
    "          action = input()\n",
    "          self.memory.append(input_ + action)\n",
    "          self.humanTurnsRem -= 1\n",
    "          return action\n",
    "\n",
    "        new_tokens = 0\n",
    "        next_token = None\n",
    "        \n",
    "        while new_tokens == 0 or (new_tokens < 20 and \"\\n\" not in self.tokenizer.decode(next_token) \n",
    "                                  and next_token != self.tokenizer.eos_token):\n",
    "          # run model\n",
    "          with torch.no_grad():\n",
    "                # get logits, only get last value\n",
    "                lmOut = self.model(input_ids, output_hidden_states=True)\n",
    "                # print(dir(lmOut))\n",
    "                logits, hidden_state = lmOut.logits, lmOut.hidden_states[-1]\n",
    "                \n",
    "                # hidden_state = hidden_state[-1]\n",
    "                values = self.valueHead(hidden_state)\n",
    "                \n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=0, top_p=1)\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "                \n",
    "                new_tokens += 1\n",
    "        \n",
    "        action_tens = input_ids[:,-new_tokens:]\n",
    "        prompt_tens = input_ids[:,:-new_tokens]\n",
    "\n",
    "        action = self.tokenizer.decode(action_tens[0,:])\n",
    "\n",
    "        print(\"action\")\n",
    "        print(action)\n",
    "        \n",
    "        # only grab last token\n",
    "        values = values[0,-1,0]\n",
    "        print(\"last token value in action\")\n",
    "        print(values)\n",
    "\n",
    "        self.memory.append(input_ + action)\n",
    "        \n",
    "        if self.mode == \"train\" and not done:\n",
    "            self.transitions.append([None, prompt_tens, action_tens, values])  # Reward will be set on the next call\n",
    "            \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "            self.memory.clear()\n",
    "            self.clearTextWorldArt = True\n",
    "            self.humanTurnsRem = self.humanTurns\n",
    "            \n",
    "        return action\n",
    "        \n",
    "        # moved to report score\n",
    "#         if self.mode == \"test\":\n",
    "#             # if done:\n",
    "#             #     self.model.reset_hidden(1)\n",
    "#             return action\n",
    "        \n",
    "#         if self.transitions:\n",
    "#             reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "#             self.last_score = score\n",
    "#             if infos[\"won\"]:\n",
    "#                 reward += 100\n",
    "#             if infos[\"lost\"]:\n",
    "#                 reward -= 100\n",
    "                \n",
    "#             self.transitions[-1][0] = reward  # Update reward information. Was initialized as none\n",
    "        \n",
    "#         self.no_train_step += 1\n",
    "        \n",
    "#         self.stats[\"max\"][\"score\"].append(score)\n",
    "#         if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "#             # get discounted returns and advantages across multiple actions. Currently not used\n",
    "#             returns, advantages = self._discount_rewards(values)\n",
    "\n",
    "#             query = []\n",
    "#             response = []\n",
    "#             rewardList = []\n",
    "\n",
    "#             for t in reversed(range(len(self.transitions))):\n",
    "#               rewards, prompt, action, values = self.transitions[t]\n",
    "\n",
    "#               query.append(prompt[0])\n",
    "#               response.append(action[0])\n",
    "#               rewardList.append(rewards)\n",
    "\n",
    "#             train_stats = self.ppo_trainer.step(query, response, rewardList)\n",
    "\n",
    "#             if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "#               print(train_stats)\n",
    "            \n",
    "#             self.transitions = []\n",
    "            \n",
    "            \n",
    "#         else:\n",
    "#             # Keep information about transitions for ppo\n",
    "            \n",
    "#             self.transitions.append([None, prompt_tens, action_tens, values])  # Reward will be set on the next call\n",
    "        \n",
    "#         if done:\n",
    "#             self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "#             self.memory.clear()\n",
    "#             self.clearTextWorldArt = True\n",
    "#             self.humanTurnsRem = self.humanTurns\n",
    "        \n",
    "#         return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1661203668768,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "3PQ5YwDMI0Vi",
    "outputId": "110d4e2e-5014-4067-a019-25f23fd70bd8"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3883,
     "status": "ok",
     "timestamp": 1661203672647,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "NlEgcVAuIQWo",
    "outputId": "9be1d2f6-2de4-4d00-ed5d-7ced9aa2563e"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15426,
     "status": "ok",
     "timestamp": 1661203688056,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "6hnyEtRaDJqg",
    "outputId": "8120b492-b50a-4a5b-d25a-204cd0fc2e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueHead(\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (activation): Identity()\n",
      "  (first_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (last_dropout): Identity()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "# from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "# from trl.ppo import PPOTrainer\n",
    "import trl\n",
    "\n",
    "import importlib\n",
    "importlib.reload(trl)\n",
    "from trl.ppoValHead import PPOTrainer\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# get models\n",
    "# gpt2_model = GPT2HeadWithValueModel.from_pretrained('gpt2')\n",
    "# gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_model_ref = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "summary(gpt2_model)\n",
    "\n",
    "agent = NLPAgent(gpt2_model, gpt2_model_ref, gpt2_tokenizer, humanTurns=0)\n",
    "print(agent.ppo_trainer.valueHead)\n",
    "\n",
    "# model_name = r\"gpt2-xl\"\n",
    "\n",
    "# Note that if you trace other models, you should set noise_level appropriately.\n",
    "# (We use 0.03 for gpt-neox-20b and 0.025 for gpt-j-6b)\n",
    "# model_name = r\"EleutherAI/gpt-neox-20b\"\n",
    "# model_name = r\"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# torch_dtype = torch.float16 if '20b' in model_name else None\n",
    "\n",
    "# mt = ModelAndTokenizer(model_name, low_cpu_mem_usage=IS_COLAB, torch_dtype=torch_dtype)\n",
    "# predict_token(mt, ['Megan Rapinoe plays the sport of',\n",
    "#                    'The Space Needle is in the city of'\n",
    "#                   ], return_p=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkN6y-r2yE6C"
   },
   "source": [
    "### Training the neural agent\n",
    "Let's first evaluate the agent before training to get a sense of its initial performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "error",
     "timestamp": 1661203726351,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "_HyFeWb8yE6D",
    "outputId": "4389785f-ffe0-40a4-c370-9387517b982b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 100 games\n",
      "['./training_games/tw-simple-rDense+gDetailed+train-house-GP-Q9nDu630U5j3tqBG.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-Oq9ns8K0uK5EhJvr.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-DWvMiBQvCR5sNkx.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-xQNoFqlDs10nfr79.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-6VXXCVGKiEWkhPer.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-R2bxU9dJUK0LCxk0.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-EloyS7BZs8LRHNVn.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-dZ2oU2KnflPksnEY.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-6kRniM7gfXQNCyj6.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-G6a7HeZPcJNGhLQ2.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-R7y2UJkBUeEqS8Bk.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-8koWh6KqcgNPI8Ox.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-o7PNc5j0URa5HYLb.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-gkdXTMLPtKpeSY5J.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-GN1YHrQ6s6vBTnNb.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-XElPhYRjT8gWs1eM.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-kGDLudo8fXeJH1Yg.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-D3PEFjpNIQykuqra.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-nOVQCNlrINVxIDje.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-7yGrcV9pTE8DF75n.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-0lvotxEKtYZ6umyJ.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-535aCDxgu5MqF7R1.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-2OOjSlaNUBKcgLy.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-165ai83atZWatMRa.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-eRNBC9q7tr5Vi3Mv.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-5PBeh9J3IX5XUjL3.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-5rjrFkZEiyOIj1y.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-gPyQi5vkhdQOUV0J.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-KrNpTrdMtdqVUKOB.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-nrEoHEqgUba7U1nx.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-1jr7FeK9TgZCQ9n.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-El9oUXJWIYVgF1jy.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-7KpYUDDdckE0cBqZ.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-W1qJibX5FqLRS3kL.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-VLpEiW2msKJpTZ7J.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-566vUvgjfXgU9GK.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-67XKC3EyH2riOk8.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-redEHVr6CmKYhrJg.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-D8gMTlO8cPoEtgZx.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-6GMVtjVYF5QRupyN.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-1vbrCekqT6xMcMdP.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-3bj9creDs3V0IKKN.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-5VVVSK6fGjlspnj.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-YnvLs6vYIWvWC51e.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-pGedtxVJsYDxsGaV.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-RvQYhbdKfMyqS5Wp.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-o8WVtobyuRR0Uqv5.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-M7MmCGG5i6kES1kV.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-ek06H8B7uqoYFVEy.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-QlDlS6DxCMnVu81D.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-6yPJsEJMTeR6fpPR.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-M2vecROLFVBxHBvl.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-2KK5iXD8ueb3ikl6.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-P6RdC912uMrbcXBX.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-M8xnUOBkulX7HNQX.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-eBxGu3KeiYjbHDl9.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-0a72tMGVtQnPSvMR.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-DvPBs6OphW6nCnQ3.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-WxnkhG07upKRhbNV.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-JBP7TQZ6fV7biZ9q.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-jYPJukgls6oZf6qG.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-mNB2tM2ohGmRIB2J.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-mbJLuQBoCbbkUv1n.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-Mj8NTxYWso7LfmN9.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-WvgBsoOmhDObfRgL.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-1MB8fmosEL9HNEv.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-Qbq3h3VWFkPdtogB.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-1EQPheDOiVB8I7m5.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-JGJdTBvVHrpBiVy5.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-qZDfWDNTJmQiLvq.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-Eykru3m8ilOGiBnx.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-p1RLFX7MU7KjFy0d.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-mn3JuMrnfPNNI5K5.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-eDi7Z2iEJ7FdL9.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-1dgPTd1ki5kQUkrn.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-WrmvTdgGc05rh3eZ.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-xqa9SlmxsDmVs8yb.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-Kx32iybbtDGRFQl6.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-n3OYtnV7IY8rTkBq.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-o2RVTmrEi6R5T3p0.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-g0GEcMQ5CWaeF3nJ.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-J9ylcQmmc190Cgn3.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-q3rdiVK5I1lrsNpj.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-jROVIEqEIya6Tr0L.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-aMq9U18aFMlOHVEe.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-ekDZtbGXIbO5FKp8.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-XR5RUnmMIgOnCYyy.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-6GZ3CqJvCX9rfev3.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-Mn8oTkr2fvv8TX1.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-aernSYY1f7rrflvB.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-OWRZsvM8h6XyIpQy.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-NPQ8TkJ9i2x6fYDM.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-3qLvIOZ9c66Igby.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-KJODI168SvJVFM9x.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-6WNBiZyofr8mu6MG.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-68kvf8x7TBd9Iq0P.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-E5eLHkaXFk6BSgR1.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-1QKVfg6YhRb1ul1e.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-vKm0IdN8c0VRfJl1.z8', './training_games/tw-simple-rDense+gDetailed+train-house-GP-mDjNimx7S5a5tmZ7.z8']\n",
      "tw-v1\n",
      "prompt tokens:  torch.Size([1, 411])\n",
      "$$$$$$$ \n",
      "\n",
      "I hope you're ready to go into rooms and interact with objects, because you've just entered TextWorld! First, it would be good if you could ensure that the chest drawer is open. After pulling open the chest drawer, retrieve the old key from the chest drawer inside the bedroom. If you have taken the old key, check that the wooden door within the bedroom is unlocked with the old key. Then, doublecheck that the wooden door is open. And then, attempt to go to the east. Okay, and then, ensure that the refrigerator is open. And then, recover the milk from the refrigerator. After that, put the milk on the stove inside the kitchen. Alright, thanks!\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents there.\n",
      "\n",
      "last token value in action\n",
      "tensor(3.0608)\n",
      "prompt tokens:  torch.Size([1, 592])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(0.1454)\n",
      "prompt tokens:  torch.Size([1, 773])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(0.7162)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(-0.3087)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(-0.0158)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(-0.2087)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(1.2632)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(-0.2998)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(-0.0485)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(-0.0860)\n",
      "{'objective/kl': array(0., dtype=float32), 'objective/kl_dist': [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])], 'objective/logprobs': [tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0061, -0.0087, -0.0028, -0.0017, -0.0002, -0.0015, -0.0008, -0.0003,\n",
      "        -0.0021, -0.0008, -0.0010, -0.0012, -0.0008, -0.0068, -0.0014, -0.0052,\n",
      "        -0.0711, -0.0016, -0.0316]), tensor([-1.1151e-02, -1.4422e-02, -7.6143e-03, -4.6753e-03, -2.8985e-03,\n",
      "        -3.1534e-03, -1.0146e-03, -2.2802e-04, -2.8525e-03, -8.5413e-04,\n",
      "        -6.3661e-04, -1.1307e-03, -1.2384e-03, -5.5016e-02, -1.2072e-03,\n",
      "        -5.1973e-03, -3.1744e+00, -2.3402e-03, -7.3483e-02]), tensor([-3.2138, -3.3990, -8.5972, -5.0392, -4.6885, -5.5400, -8.8361, -9.2784,\n",
      "        -1.1101, -2.8854, -3.0733, -4.2448, -0.8737, -7.8494, -2.1019, -5.3068,\n",
      "        -6.5258, -1.6473, -0.8370])], 'objective/ref_logprobs': [tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0071, -0.0080, -0.0022, -0.0014, -0.0001, -0.0016, -0.0017, -0.0006,\n",
      "        -0.0027, -0.0011, -0.0018, -0.0013, -0.0008, -0.0083, -0.0016, -0.0066,\n",
      "        -0.0285, -0.0011, -0.0255]), tensor([-0.0061, -0.0087, -0.0028, -0.0017, -0.0002, -0.0015, -0.0008, -0.0003,\n",
      "        -0.0021, -0.0008, -0.0010, -0.0012, -0.0008, -0.0068, -0.0014, -0.0052,\n",
      "        -0.0711, -0.0016, -0.0316]), tensor([-1.1151e-02, -1.4422e-02, -7.6143e-03, -4.6753e-03, -2.8985e-03,\n",
      "        -3.1534e-03, -1.0146e-03, -2.2802e-04, -2.8525e-03, -8.5413e-04,\n",
      "        -6.3661e-04, -1.1307e-03, -1.2384e-03, -5.5016e-02, -1.2072e-03,\n",
      "        -5.1973e-03, -3.1744e+00, -2.3402e-03, -7.3483e-02]), tensor([-3.2138, -3.3990, -8.5972, -5.0392, -4.6885, -5.5400, -8.8361, -9.2784,\n",
      "        -1.1101, -2.8854, -3.0733, -4.2448, -0.8737, -7.8494, -2.1019, -5.3068,\n",
      "        -6.5258, -1.6473, -0.8370])], 'objective/kl_coef': 0.2, 'objective/entropy': array(8.927008, dtype=float32), 'ppo/mean_non_score_reward': array(0., dtype=float32), 'ppo/loss/policy': array([0.00834939], dtype=float32), 'ppo/loss/value': array([7.262619], dtype=float32), 'ppo/loss/total': array([0.73461133], dtype=float32), 'ppo/policy/entropy': array([1.8213739], dtype=float32), 'ppo/policy/approxkl': array([0.01911533], dtype=float32), 'ppo/policy/policykl': array([-0.06667415], dtype=float32), 'ppo/policy/clipfrac': array([0.06959834]), 'ppo/policy/advantages': array([-0.23066747,  0.29877546,  0.3336502 , -0.49992168,  2.3726976 ,\n",
      "        0.1457176 , -0.58027816, -0.7581786 ,  0.07290898,  0.4442266 ,\n",
      "        0.5547598 , -1.2667551 , -0.5089547 , -1.1752613 , -0.75181985,\n",
      "        2.006355  ,  0.9937972 , -0.20238855, -1.2486619 ,  3.0281987 ,\n",
      "       -0.3066072 , -1.3357323 , -1.2768633 , -0.16793579, -0.3385478 ,\n",
      "       -0.13225847,  0.02438977,  0.85104847,  1.0510205 ,  0.5833612 ,\n",
      "       -1.2080843 ,  0.8439093 ,  0.0840499 , -0.5901352 , -0.03526555,\n",
      "       -0.4274037 , -0.51199883, -0.1351462 ,  1.2924343 ,  0.15769996,\n",
      "       -0.50654733, -0.46682367,  1.5607898 , -2.4239538 ,  0.84634095,\n",
      "        1.626183  , -0.41260934, -0.01417337, -0.02534218, -0.34653032,\n",
      "        1.4144953 , -0.12110642, -0.06951552, -0.5064705 , -0.6772727 ,\n",
      "       -1.0172886 , -0.31030923,  0.3101131 ,  2.124822  , -0.55239296,\n",
      "       -0.8485756 ,  0.09697395, -0.01693137, -0.54202104, -0.5687345 ,\n",
      "       -0.36351034,  0.15563682, -0.05068568, -0.9894938 ,  1.8322976 ,\n",
      "       -0.7974899 , -0.60633516, -0.6461151 ,  1.0476152 ,  1.7300476 ,\n",
      "       -1.315221  ,  0.49855062, -2.706405  ,  1.16022   , -0.32596222,\n",
      "        0.01552111, -0.22713405,  1.1291683 , -0.36656186,  0.35833633,\n",
      "        0.3614383 ,  2.2104363 , -0.5481163 ,  0.04954158, -0.57244945,\n",
      "        0.10299642, -0.52211356,  0.7267802 , -0.7785655 , -0.56568086,\n",
      "       -1.4028809 ,  0.11509554, -0.10138839,  0.11789344, -1.4425385 ,\n",
      "       -0.21321775,  1.1004331 , -0.19120754,  0.16604549,  0.28631464,\n",
      "        0.24496108, -0.09078677,  1.1192064 ,  0.10753684,  2.615648  ,\n",
      "       -0.03296081, -0.14708722, -0.18196596, -2.0691004 ,  2.6533077 ,\n",
      "       -0.14952117, -0.25392097, -0.1878675 ,  0.11484686, -0.33617547,\n",
      "       -0.5314534 ,  0.11241065, -1.9482551 , -0.3646363 ,  0.17116256,\n",
      "        0.07853664, -0.35392594,  0.92793006,  1.2807729 , -0.3751774 ,\n",
      "        1.1179708 , -1.4889512 , -0.46705434,  0.7010952 , -1.9314629 ,\n",
      "        0.19119814, -1.5299335 ,  1.5032284 , -0.19951531, -0.12988022,\n",
      "       -0.08965447,  1.3032923 ,  0.43750483, -0.02791995,  0.41606745,\n",
      "        1.126355  , -0.07149441,  0.17753151,  0.9544636 , -0.23213154,\n",
      "       -0.47309968, -2.1256454 ,  0.47827294,  1.2527121 ,  0.98097384,\n",
      "       -0.3141374 , -1.9296336 ,  0.84680045, -0.15197296, -0.4358588 ,\n",
      "        1.5527723 ,  0.59494233,  0.04970641, -0.06067548, -2.322762  ,\n",
      "       -0.31620443,  1.2130687 , -0.22011948, -0.10091392, -0.7414426 ,\n",
      "       -0.37552726, -0.02047939, -0.4065217 , -0.49526924, -0.04691759,\n",
      "       -0.21947157, -0.4980864 , -0.5685962 ,  1.2751539 ,  1.782581  ,\n",
      "        0.09877276, -0.19298483, -0.44110823,  0.9093985 , -0.6786444 ,\n",
      "       -0.5931026 , -0.9362419 ,  0.92642546, -2.0141432 ,  2.1192358 ,\n",
      "        0.49855062, -2.706405  ,  1.16022   , -0.32596222,  0.01552111,\n",
      "       -0.22713405,  1.1291683 , -0.36656186,  0.35833633,  0.3614383 ,\n",
      "        2.2104363 , -0.5481163 ,  0.04954158, -0.57244945,  0.10299642,\n",
      "       -0.52211356,  0.7267802 , -0.7785655 , -0.56568086, -1.4028809 ,\n",
      "        0.11509554, -0.10138839,  0.11789344, -1.4425385 , -0.21321775,\n",
      "        1.1004331 , -0.19120754,  0.16604549,  0.28631464,  0.24496108,\n",
      "       -0.09078677,  1.1192064 ,  0.10753684,  2.615648  , -0.03296081,\n",
      "       -0.14708722, -0.18196596, -2.0691004 ,  0.3101131 ,  2.124822  ,\n",
      "       -0.55239296, -0.8485756 ,  0.09697395, -0.01693137, -0.54202104,\n",
      "       -0.5687345 , -0.36351034,  0.15563682, -0.05068568, -0.9894938 ,\n",
      "        1.8322976 , -0.7974899 , -0.60633516, -0.6461151 ,  1.0476152 ,\n",
      "        1.7300476 , -1.315221  , -0.02047939, -0.4065217 , -0.49526924,\n",
      "       -0.04691759, -0.21947157, -0.4980864 , -0.5685962 ,  1.2751539 ,\n",
      "        1.782581  ,  0.09877276, -0.19298483, -0.44110823,  0.9093985 ,\n",
      "       -0.6786444 , -0.5931026 , -0.9362419 ,  0.92642546, -2.0141432 ,\n",
      "        2.1192358 ,  3.0281987 , -0.3066072 , -1.3357323 , -1.2768633 ,\n",
      "       -0.16793579, -0.3385478 , -0.13225847,  0.02438977,  0.85104847,\n",
      "        1.0510205 ,  0.5833612 , -1.2080843 ,  0.8439093 ,  0.0840499 ,\n",
      "       -0.5901352 , -0.03526555, -0.4274037 , -0.51199883, -0.1351462 ,\n",
      "        1.2924343 ,  0.15769996, -0.50654733, -0.46682367,  1.5607898 ,\n",
      "       -2.4239538 ,  0.84634095,  1.626183  , -0.41260934, -0.01417337,\n",
      "       -0.02534218, -0.34653032,  1.4144953 , -0.12110642, -0.06951552,\n",
      "       -0.5064705 , -0.6772727 , -1.0172886 , -0.31030923,  0.47827294,\n",
      "        1.2527121 ,  0.98097384, -0.3141374 , -1.9296336 ,  0.84680045,\n",
      "       -0.15197296, -0.4358588 ,  1.5527723 ,  0.59494233,  0.04970641,\n",
      "       -0.06067548, -2.322762  , -0.31620443,  1.2130687 , -0.22011948,\n",
      "       -0.10091392, -0.7414426 , -0.37552726,  0.7010952 , -1.9314629 ,\n",
      "        0.19119814, -1.5299335 ,  1.5032284 , -0.19951531, -0.12988022,\n",
      "       -0.08965447,  1.3032923 ,  0.43750483, -0.02791995,  0.41606745,\n",
      "        1.126355  , -0.07149441,  0.17753151,  0.9544636 , -0.23213154,\n",
      "       -0.47309968, -2.1256454 , -0.23066747,  0.29877546,  0.3336502 ,\n",
      "       -0.49992168,  2.3726976 ,  0.1457176 , -0.58027816, -0.7581786 ,\n",
      "        0.07290898,  0.4442266 ,  0.5547598 , -1.2667551 , -0.5089547 ,\n",
      "       -1.1752613 , -0.75181985,  2.006355  ,  0.9937972 , -0.20238855,\n",
      "       -1.2486619 ,  2.6533077 , -0.14952117, -0.25392097, -0.1878675 ,\n",
      "        0.11484686, -0.33617547, -0.5314534 ,  0.11241065, -1.9482551 ,\n",
      "       -0.3646363 ,  0.17116256,  0.07853664, -0.35392594,  0.92793006,\n",
      "        1.2807729 , -0.3751774 ,  1.1179708 , -1.4889512 , -0.46705434,\n",
      "        1.2924343 ,  0.15769996, -0.50654733, -0.46682367,  1.5607898 ,\n",
      "       -2.4239538 ,  0.84634095,  1.626183  , -0.41260934, -0.01417337,\n",
      "       -0.02534218, -0.34653032,  1.4144953 , -0.12110642, -0.06951552,\n",
      "       -0.5064705 , -0.6772727 , -1.0172886 , -0.31030923, -0.02047939,\n",
      "       -0.4065217 , -0.49526924, -0.04691759, -0.21947157, -0.4980864 ,\n",
      "       -0.5685962 ,  1.2751539 ,  1.782581  ,  0.09877276, -0.19298483,\n",
      "       -0.44110823,  0.9093985 , -0.6786444 , -0.5931026 , -0.9362419 ,\n",
      "        0.92642546, -2.0141432 ,  2.1192358 ,  0.49855062, -2.706405  ,\n",
      "        1.16022   , -0.32596222,  0.01552111, -0.22713405,  1.1291683 ,\n",
      "       -0.36656186,  0.35833633,  0.3614383 ,  2.2104363 , -0.5481163 ,\n",
      "        0.04954158, -0.57244945,  0.10299642, -0.52211356,  0.7267802 ,\n",
      "       -0.7785655 , -0.56568086,  0.7010952 , -1.9314629 ,  0.19119814,\n",
      "       -1.5299335 ,  1.5032284 , -0.19951531, -0.12988022, -0.08965447,\n",
      "        1.3032923 ,  0.43750483, -0.02791995,  0.41606745,  1.126355  ,\n",
      "       -0.07149441,  0.17753151,  0.9544636 , -0.23213154, -0.47309968,\n",
      "       -2.1256454 ,  3.0281987 , -0.3066072 , -1.3357323 , -1.2768633 ,\n",
      "       -0.16793579, -0.3385478 , -0.13225847,  0.02438977,  0.85104847,\n",
      "        1.0510205 ,  0.5833612 , -1.2080843 ,  0.8439093 ,  0.0840499 ,\n",
      "       -0.5901352 , -0.03526555, -0.4274037 , -0.51199883, -0.1351462 ,\n",
      "       -1.4028809 ,  0.11509554, -0.10138839,  0.11789344, -1.4425385 ,\n",
      "       -0.21321775,  1.1004331 , -0.19120754,  0.16604549,  0.28631464,\n",
      "        0.24496108, -0.09078677,  1.1192064 ,  0.10753684,  2.615648  ,\n",
      "       -0.03296081, -0.14708722, -0.18196596, -2.0691004 ,  0.3101131 ,\n",
      "        2.124822  , -0.55239296, -0.8485756 ,  0.09697395, -0.01693137,\n",
      "       -0.54202104, -0.5687345 , -0.36351034,  0.15563682, -0.05068568,\n",
      "       -0.9894938 ,  1.8322976 , -0.7974899 , -0.60633516, -0.6461151 ,\n",
      "        1.0476152 ,  1.7300476 , -1.315221  ,  0.47827294,  1.2527121 ,\n",
      "        0.98097384, -0.3141374 , -1.9296336 ,  0.84680045, -0.15197296,\n",
      "       -0.4358588 ,  1.5527723 ,  0.59494233,  0.04970641, -0.06067548,\n",
      "       -2.322762  , -0.31620443,  1.2130687 , -0.22011948, -0.10091392,\n",
      "       -0.7414426 , -0.37552726, -0.23066747,  0.29877546,  0.3336502 ,\n",
      "       -0.49992168,  2.3726976 ,  0.1457176 , -0.58027816, -0.7581786 ,\n",
      "        0.07290898,  0.4442266 ,  0.5547598 , -1.2667551 , -0.5089547 ,\n",
      "       -1.1752613 , -0.75181985,  2.006355  ,  0.9937972 , -0.20238855,\n",
      "       -1.2486619 ,  2.6533077 , -0.14952117, -0.25392097, -0.1878675 ,\n",
      "        0.11484686, -0.33617547, -0.5314534 ,  0.11241065, -1.9482551 ,\n",
      "       -0.3646363 ,  0.17116256,  0.07853664, -0.35392594,  0.92793006,\n",
      "        1.2807729 , -0.3751774 ,  1.1179708 , -1.4889512 , -0.46705434,\n",
      "        3.0281987 , -0.3066072 , -1.3357323 , -1.2768633 , -0.16793579,\n",
      "       -0.3385478 , -0.13225847,  0.02438977,  0.85104847,  1.0510205 ,\n",
      "        0.5833612 , -1.2080843 ,  0.8439093 ,  0.0840499 , -0.5901352 ,\n",
      "       -0.03526555, -0.4274037 , -0.51199883, -0.1351462 ,  0.47827294,\n",
      "        1.2527121 ,  0.98097384, -0.3141374 , -1.9296336 ,  0.84680045,\n",
      "       -0.15197296, -0.4358588 ,  1.5527723 ,  0.59494233,  0.04970641,\n",
      "       -0.06067548, -2.322762  , -0.31620443,  1.2130687 , -0.22011948,\n",
      "       -0.10091392, -0.7414426 , -0.37552726,  0.7010952 , -1.9314629 ,\n",
      "        0.19119814, -1.5299335 ,  1.5032284 , -0.19951531, -0.12988022,\n",
      "       -0.08965447,  1.3032923 ,  0.43750483, -0.02791995,  0.41606745,\n",
      "        1.126355  , -0.07149441,  0.17753151,  0.9544636 , -0.23213154,\n",
      "       -0.47309968, -2.1256454 ,  2.6533077 , -0.14952117, -0.25392097,\n",
      "       -0.1878675 ,  0.11484686, -0.33617547, -0.5314534 ,  0.11241065,\n",
      "       -1.9482551 , -0.3646363 ,  0.17116256,  0.07853664, -0.35392594,\n",
      "        0.92793006,  1.2807729 , -0.3751774 ,  1.1179708 , -1.4889512 ,\n",
      "       -0.46705434,  1.2924343 ,  0.15769996, -0.50654733, -0.46682367,\n",
      "        1.5607898 , -2.4239538 ,  0.84634095,  1.626183  , -0.41260934,\n",
      "       -0.01417337, -0.02534218, -0.34653032,  1.4144953 , -0.12110642,\n",
      "       -0.06951552, -0.5064705 , -0.6772727 , -1.0172886 , -0.31030923,\n",
      "       -0.02047939, -0.4065217 , -0.49526924, -0.04691759, -0.21947157,\n",
      "       -0.4980864 , -0.5685962 ,  1.2751539 ,  1.782581  ,  0.09877276,\n",
      "       -0.19298483, -0.44110823,  0.9093985 , -0.6786444 , -0.5931026 ,\n",
      "       -0.9362419 ,  0.92642546, -2.0141432 ,  2.1192358 ,  0.3101131 ,\n",
      "        2.124822  , -0.55239296, -0.8485756 ,  0.09697395, -0.01693137,\n",
      "       -0.54202104, -0.5687345 , -0.36351034,  0.15563682, -0.05068568,\n",
      "       -0.9894938 ,  1.8322976 , -0.7974899 , -0.60633516, -0.6461151 ,\n",
      "        1.0476152 ,  1.7300476 , -1.315221  , -0.23066747,  0.29877546,\n",
      "        0.3336502 , -0.49992168,  2.3726976 ,  0.1457176 , -0.58027816,\n",
      "       -0.7581786 ,  0.07290898,  0.4442266 ,  0.5547598 , -1.2667551 ,\n",
      "       -0.5089547 , -1.1752613 , -0.75181985,  2.006355  ,  0.9937972 ,\n",
      "       -0.20238855, -1.2486619 , -1.4028809 ,  0.11509554, -0.10138839,\n",
      "        0.11789344, -1.4425385 , -0.21321775,  1.1004331 , -0.19120754,\n",
      "        0.16604549,  0.28631464,  0.24496108, -0.09078677,  1.1192064 ,\n",
      "        0.10753684,  2.615648  , -0.03296081, -0.14708722, -0.18196596,\n",
      "       -2.0691004 ,  0.49855062, -2.706405  ,  1.16022   , -0.32596222,\n",
      "        0.01552111, -0.22713405,  1.1291683 , -0.36656186,  0.35833633,\n",
      "        0.3614383 ,  2.2104363 , -0.5481163 ,  0.04954158, -0.57244945,\n",
      "        0.10299642, -0.52211356,  0.7267802 , -0.7785655 , -0.56568086],\n",
      "      dtype=float32), 'ppo/policy/advantages_mean': array([1.2254247e-09], dtype=float32), 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.1571544 ,\n",
      "       1.0928807 , 1.0023507 , 0.97727996, 1.0721525 , 1.0493877 ,\n",
      "       1.0604081 , 0.9305553 , 1.0098505 , 0.98226696, 0.98926055,\n",
      "       0.9494465 , 0.966376  , 0.94465655, 0.97278756, 0.92105037,\n",
      "       1.0708027 , 0.9316347 , 1.004058  , 1.0030721 , 1.0045443 ,\n",
      "       1.0010312 , 1.000315  , 0.9999697 , 0.99996287, 0.99997336,\n",
      "       1.0000511 , 1.000434  , 0.99999464, 0.99951464, 1.0001132 ,\n",
      "       1.0001087 , 1.0007546 , 1.000135  , 0.99909776, 1.0021063 ,\n",
      "       0.99989617, 0.9983201 , 1.0032508 , 1.0062433 , 1.0015901 ,\n",
      "       1.0006691 , 1.0000689 , 1.0000614 , 0.9999103 , 0.9999926 ,\n",
      "       1.0004551 , 1.0000049 , 0.9998407 , 1.0002553 , 1.0001091 ,\n",
      "       0.9987232 , 1.0002092 , 0.9981322 , 0.9829041 , 0.9993019 ,\n",
      "       0.97229546, 1.0036117 , 1.0055395 , 1.0013014 , 1.0005364 ,\n",
      "       0.9999547 , 0.99991333, 1.0003805 , 1.0001333 , 1.0005376 ,\n",
      "       1.0001203 , 1.0002196 , 1.000284  , 1.000121  , 0.9997571 ,\n",
      "       1.0003775 , 0.9980882 , 1.0125867 , 0.9991894 , 0.9686367 ,\n",
      "       1.0039738 , 1.0057675 , 1.0013313 , 1.0005811 , 0.99994814,\n",
      "       0.9997222 , 1.000546  , 1.0001658 , 1.0004696 , 1.0001229 ,\n",
      "       1.000658  , 1.0002514 , 0.9998405 , 0.99378175, 1.0004463 ,\n",
      "       0.9970604 , 1.017169  , 0.9988558 , 0.9433882 , 1.0041957 ,\n",
      "       1.0058222 , 1.0012811 , 1.0005863 , 0.9999362 , 0.99926734,\n",
      "       1.0006113 , 1.0001808 , 1.0003098 , 1.0000379 , 1.0009377 ,\n",
      "       1.0001013 , 0.9989848 , 0.97913724, 1.0004376 , 0.9955861 ,\n",
      "       1.0190748 , 0.99089706, 0.9118138 , 1.0042503 , 1.0057273 ,\n",
      "       1.0012534 , 1.0005741 , 0.9999137 , 0.9984094 , 1.000617  ,\n",
      "       1.0001787 , 1.0001764 , 0.99980223, 1.0010606 , 0.9997599 ,\n",
      "       0.99738646, 0.9316791 , 1.0003352 , 0.99330455, 1.0188705 ,\n",
      "       0.9823046 , 0.8709175 , 1.0048475 , 1.0118461 , 1.0030966 ,\n",
      "       1.0034906 , 1.0023147 , 0.99903065, 0.99966997, 0.99993765,\n",
      "       1.0011642 , 0.99940586, 0.99970204, 0.99707174, 0.99711066,\n",
      "       0.8862793 , 0.99978024, 0.9931902 , 0.7173727 , 0.9776812 ,\n",
      "       0.2172064 , 1.002915  , 1.0006373 , 0.99999523, 1.0001273 ,\n",
      "       0.9997645 , 0.9953201 , 1.0003479 , 1.0001503 , 0.9998538 ,\n",
      "       0.9969841 , 1.0006704 , 0.9977379 , 0.98211646, 0.6873405 ,\n",
      "       0.9994318 , 0.9790772 , 1.0136395 , 0.96770966, 0.82614243,\n",
      "       1.0020813 , 0.96516347, 0.9945596 , 0.99859214, 0.99940866,\n",
      "       0.9883185 , 0.9996802 , 1.0000894 , 0.9997652 , 0.9888824 ,\n",
      "       0.99978465, 0.995036  , 0.97525704, 0.63077295, 0.9986054 ,\n",
      "       0.9846319 , 1.0015775 , 0.95161927, 0.8225692 , 0.9719475 ,\n",
      "       0.89497775, 0.97090054, 0.99320453, 0.9973773 , 0.98082197,\n",
      "       0.9923579 , 0.99989396, 0.9998532 , 0.94220006, 0.9900832 ,\n",
      "       0.9756519 , 0.9486751 , 0.5196152 , 0.99721545, 0.9684306 ,\n",
      "       0.9830612 , 0.9307163 , 0.81758034, 0.9601264 , 0.85213447,\n",
      "       0.96179664, 0.98991853, 0.997112  , 0.9833127 , 0.979581  ,\n",
      "       0.99972826, 1.0002279 , 0.9717602 , 0.9979067 , 0.9892909 ,\n",
      "       0.9549785 , 0.55043924, 0.9975623 , 0.9556372 , 0.9805107 ,\n",
      "       0.91236633, 0.7769368 , 0.9610763 , 0.82723707, 0.964061  ,\n",
      "       0.98938173, 0.9971351 , 0.97830105, 0.98972017, 0.99993557,\n",
      "       0.999946  , 0.9462323 , 0.99828035, 0.97977537, 0.9429796 ,\n",
      "       0.532488  , 0.9975786 , 0.9613288 , 0.9793831 , 0.92340463,\n",
      "       0.7987754 , 2.0184875 , 0.23739366, 0.7659972 , 0.43972567,\n",
      "       1.1412226 , 0.8004189 , 2.950141  , 0.98795384, 0.7316765 ,\n",
      "       0.44880384, 2.8500943 , 0.6669929 , 0.83071214, 0.6444904 ,\n",
      "       0.64357096, 0.84085584, 1.2075931 , 1.2902522 , 0.8551577 ,\n",
      "       0.96209127, 0.8891661 , 0.9748576 , 0.9996687 , 0.99944496,\n",
      "       0.9904544 , 0.99949753, 1.0001317 , 0.9998489 , 0.9889418 ,\n",
      "       0.9999999 , 0.9951708 , 0.9626691 , 0.6770831 , 0.9975831 ,\n",
      "       0.97914433, 0.9914086 , 0.9263082 , 0.7821189 , 0.9023006 ,\n",
      "       0.70910233, 0.93787533, 0.9912937 , 0.99666125, 0.9854045 ,\n",
      "       0.9766245 , 0.998988  , 1.0012705 , 0.9838091 , 0.988455  ,\n",
      "       0.97701925, 0.94381475, 0.6151835 , 0.997691  , 0.9741225 ,\n",
      "       1.8209951 , 0.88706124, 0.5233129 , 0.96358967, 0.920242  ,\n",
      "       0.98076665, 1.0005893 , 0.9997332 , 0.9969251 , 1.0001258 ,\n",
      "       1.0002098 , 1.0001986 , 0.99786055, 1.0004535 , 0.9971524 ,\n",
      "       0.9708854 , 0.77093834, 0.99878407, 0.98263866, 1.0000325 ,\n",
      "       0.9310925 , 0.77489865, 0.96504426, 0.92951375, 0.9835156 ,\n",
      "       1.0007157 , 0.9997861 , 0.9979743 , 1.0002503 , 1.0002356 ,\n",
      "       1.0004197 , 0.99859124, 1.0005301 , 0.9976815 , 0.97337145,\n",
      "       0.80601364, 0.99915284, 0.9816829 , 0.99828184, 0.9334903 ,\n",
      "       0.7771304 , 0.96615416, 0.9342947 , 0.9854349 , 1.0007848 ,\n",
      "       0.99981695, 0.9979237 , 1.0003114 , 1.0002486 , 1.0006089 ,\n",
      "       0.99907035, 1.0005692 , 0.99795854, 0.97465897, 0.8295526 ,\n",
      "       0.99936795, 0.9814095 , 0.99412024, 0.93445253, 0.783039  ,\n",
      "       0.96654916, 0.93547547, 0.9863393 , 1.0008252 , 0.99983543,\n",
      "       0.9979029 , 1.0003412 , 1.0002539 , 1.0007712 , 0.99931896,\n",
      "       1.0005901 , 0.99812156, 0.9751162 , 0.8447394 , 0.9994665 ,\n",
      "       0.98194635, 0.9929965 , 0.9332514 , 0.7904998 , 0.9662688 ,\n",
      "       0.9342429 , 0.98669684, 1.0008506 , 0.9998489 , 0.99797726,\n",
      "       1.000349  , 1.0002524 , 1.0009052 , 0.9994417 , 1.0005965 ,\n",
      "       0.9982114 , 0.97506255, 0.85447043, 0.99951774, 0.9825631 ,\n",
      "       0.9922274 , 0.9311219 , 0.7996898 , 0.96523964, 0.9319476 ,\n",
      "       0.986848  , 1.0008687 , 0.9998582 , 0.9981693 , 1.0003533 ,\n",
      "       1.00025   , 1.0010061 , 0.9994616 , 1.0005876 , 0.9982375 ,\n",
      "       0.9743556 , 0.8609862 , 0.99960285, 0.983114  , 0.9912009 ,\n",
      "       0.92848283, 0.81015664, 0.9614441 , 0.92247164, 0.9862866 ,\n",
      "       1.0008831 , 0.9998638 , 0.99856603, 1.0003186 , 1.0002348 ,\n",
      "       1.0011256 , 0.99928224, 1.0005445 , 0.9981049 , 0.9694493 ,\n",
      "       0.8557249 , 0.9995033 , 0.98474705, 0.9825781 , 0.91712826,\n",
      "       0.8138636 , 0.9555544 , 0.25333682, 1.4167153 , 0.5611059 ,\n",
      "       1.3217541 , 0.89178896, 0.96303517, 1.2398763 , 2.6430728 ,\n",
      "       0.55156296, 0.71657884, 0.65600556, 0.7546991 , 1.2051086 ,\n",
      "       0.8089793 , 0.8108703 , 0.57670295, 1.1264291 , 0.8236712 ,\n",
      "       0.9480343 , 0.8995619 , 0.9842606 , 1.0009128 , 0.9998649 ,\n",
      "       0.99749845, 1.0001765 , 1.0001658 , 1.0012798 , 0.9987523 ,\n",
      "       1.000365  , 0.9975912 , 0.956057  , 0.83025044, 0.9989935 ,\n",
      "       0.98021406, 0.9670024 , 0.889305  , 0.8194511 , 0.93275255,\n",
      "       0.87801915, 0.9801864 , 1.0011225 , 0.9999169 , 0.9939524 ,\n",
      "       0.9989273 , 0.99980414, 1.0011102 , 0.9947529 , 0.99904644,\n",
      "       0.9943698 , 0.95128477, 0.7661552 , 0.998025  , 0.96530414,\n",
      "       0.9161953 , 0.8462434 , 0.7503466 , 0.79704016, 0.7887487 ,\n",
      "       0.94894075, 1.0025567 , 1.000265  , 0.9812798 , 0.9799046 ,\n",
      "       0.9977388 , 1.0017502 , 0.98544055, 0.98804116, 0.9800669 ,\n",
      "       0.93364733, 0.78024536, 0.9981357 , 0.97543776, 0.4474094 ,\n",
      "       0.85641336, 0.5681142 , 0.9224232 , 0.84973633, 0.9767122 ,\n",
      "       1.0008681 , 0.99973404, 0.97680926, 0.9984945 , 0.99972606,\n",
      "       1.0012606 , 0.96867025, 0.9985312 , 0.99260426, 0.90824   ,\n",
      "       0.7524973 , 0.9987246 , 0.9578657 , 0.9234003 , 0.8215727 ,\n",
      "       0.8247429 , 0.9166093 , 0.8435576 , 0.9743242 , 1.0008453 ,\n",
      "       0.9996763 , 0.9680944 , 0.99767154, 0.9995051 , 1.0012941 ,\n",
      "       0.9570944 , 0.9952782 , 0.987046  , 0.8921288 , 0.741263  ,\n",
      "       0.9971771 , 0.9512078 , 0.90974635, 0.8042833 , 0.829178  ,\n",
      "       0.9107524 , 0.31675377, 0.6796975 , 0.63338614, 0.4008569 ,\n",
      "       0.964331  , 0.80546445, 0.43413273, 2.3700557 , 0.67515695,\n",
      "       0.7563141 , 1.6000099 , 0.7859631 , 0.73512274, 0.8197501 ,\n",
      "       0.835772  , 0.65006113, 1.0955569 , 0.90911186, 0.7707064 ,\n",
      "       0.8234667 , 0.93406427, 1.0021024 , 0.9982761 , 0.9602958 ,\n",
      "       0.944395  , 0.9939125 , 1.0016537 , 0.97676516, 0.9805824 ,\n",
      "       0.9783833 , 0.90092   , 0.7890925 , 0.99698013, 0.96478516,\n",
      "       0.73050463, 0.8082649 , 0.6126341 , 0.9016453 , 0.8262295 ,\n",
      "       0.9679457 , 1.0007268 , 0.9993874 , 0.93239963, 0.9593987 ,\n",
      "       0.99810666, 1.0011603 , 0.91924506, 0.9756533 , 0.97634655,\n",
      "       0.8474027 , 0.726371  , 0.9828835 , 0.9245636 , 0.8624619 ,\n",
      "       0.7565552 , 0.83667755, 0.8972003 , 0.81941324, 0.9658768 ,\n",
      "       1.0006546 , 0.9992062 , 0.9162939 , 0.9377261 , 0.997084  ,\n",
      "       1.0010551 , 0.8903687 , 0.9702788 , 0.97400266, 0.83186364,\n",
      "       0.72614163, 0.9797152 , 0.9134965 , 0.84882903, 0.74563164,\n",
      "       0.8383553 , 0.89424926, 0.8150711 , 0.9650818 , 1.0005788 ,\n",
      "       0.99901134, 0.90031433, 0.9260162 , 0.9959561 , 1.000957  ,\n",
      "       0.86064357, 0.9660309 , 0.9720751 , 0.81965053, 0.73480594,\n",
      "       0.97713923, 0.90398675, 0.8389904 , 0.7402814 , 0.8404722 ,\n",
      "       0.89216137, 0.8118651 , 0.9647919 , 1.0005014 , 0.99882925,\n",
      "       0.8851795 , 0.91740245, 0.99461454, 1.0008576 , 0.8305768 ,\n",
      "       0.9621664 , 0.97045845, 0.8103337 , 0.7500886 , 0.9749401 ,\n",
      "       0.8959112 , 0.8315924 , 0.73934424, 0.8430234 , 0.901212  ,\n",
      "       0.8743001 , 0.96937144, 1.0007478 , 0.99912703, 0.9208264 ,\n",
      "       0.93389714, 0.9944432 , 1.0005666 , 0.89923334, 0.96375644,\n",
      "       0.97110486, 0.84729004, 0.7806034 , 0.9738907 , 0.8978862 ,\n",
      "       0.76874685, 0.7279111 , 0.7772704 , 0.88918513, 0.8054898 ,\n",
      "       0.96417344, 1.0002866 , 0.9983729 , 0.85214204, 0.9019711 ,\n",
      "       0.9902724 , 1.0006143 , 0.7570228 , 0.9495678 , 0.9665836 ,\n",
      "       0.794163  , 0.78379315, 0.96998876, 0.87909395, 0.822153  ,\n",
      "       0.74353117, 0.8483128 , 0.8887764 , 0.80335027, 0.9643835 ,\n",
      "       1.0001782 , 0.9981177 , 0.8380914 , 0.8962404 , 0.98760426,\n",
      "       1.0004736 , 0.73160076, 0.9434909 , 0.9650957 , 0.79127955,\n",
      "       0.80574256, 0.96795386, 0.8734852 , 0.82249516, 0.7503499 ,\n",
      "       0.85186565, 0.8886112 , 0.80164665, 0.96462834, 1.0000744 ,\n",
      "       0.9977916 , 0.82571   , 0.89132094, 0.9846873 , 1.0003334 ,\n",
      "       0.71548426, 0.9380317 , 0.96386695, 0.7908781 , 0.82487625,\n",
      "       0.96616375, 0.8690094 , 0.8235664 , 0.7587197 , 0.8551576 ],\n",
      "      dtype=float32), 'ppo/returns/mean': array([1.1960905], dtype=float32), 'ppo/returns/var': array([0.48336878], dtype=float32), 'ppo/val/vpred': array([1.1318238], dtype=float32), 'ppo/val/error': array([4.3265524], dtype=float32), 'ppo/val/clipfrac': array([0.75263158]), 'ppo/val/mean': array([3.7259345], dtype=float32), 'ppo/val/var': array([7.0038705], dtype=float32), 'ppo/val/var_explained': array([-7.9508314], dtype=float32), 'time/ppo/forward_pass': 27.72282338142395, 'time/ppo/compute_rewards': 0.00033926963806152344, 'time/ppo/optimize_step': 183.5927610397339, 'time/ppo/calc_stats': 0.020581722259521484, 'time/ppo/total': 211.35285902023315}\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can readenglish English bookmark on page 7\n",
      "\n",
      "last token value in action\n",
      "tensor(1.1628)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can analyse oax ha...'s to do to his own selling.\n",
      "\n",
      "last token value in action\n",
      "tensor(1.6831)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats, or donate to my people, if u properly are\n",
      "last token value in action\n",
      "tensor(2.7931)\n",
      "prompt tokens:  torch.Size([1, 962])\n",
      "You seem to want to talk to someone, but I can't see whom.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet,, (-= Me ** Oops.)\n",
      "\n",
      "last token value in action\n",
      "tensor(2.3635)\n",
      "prompt tokens:  torch.Size([1, 962])\n",
      "You seem to want to talk to someone, but I can't see whom.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats, many of which I'm of no use here.\n",
      "last token value in action\n",
      "tensor(0.4547)\n",
      "prompt tokens:  torch.Size([1, 962])\n",
      "You seem to want to talk to someone, but I can't see whom.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can hold more gold.\n",
      "\n",
      "last token value in action\n",
      "tensor(1.4707)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold than\n",
      "\n",
      "last token value in action\n",
      "tensor(3.2885)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can check the wood in the chest note, just like in Morrowind\n",
      "\n",
      "last token value in action\n",
      "tensor(1.0848)\n",
      "prompt tokens:  torch.Size([1, 962])\n",
      "You seem to want to talk to someone, but I can't see whom.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold.\n",
      "\n",
      "last token value in action\n",
      "tensor(1.1921)\n",
      "prompt tokens:  torch.Size([1, 954])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You've just sauntered into a bedroom.\n",
      "\n",
      "You make out a chest drawer. Something scurries by right in the corner of your eye. Probably nothing. You can make out a closed antique trunk. You lean against the wall, inadvertently pressing a secret button. The wall opens up to reveal a king-size bed. The king-size bed is usual. Looks like someone's already been here and taken everything off it, though.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " can also deposit more gold as backcheats to your wallet. Save the contents here.\n",
      "\n",
      "last token value in action\n",
      "tensor(1.0676)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "print(\"Training on 100 games\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
    "\n",
    "# Save the trained agent.\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POiGO-PmyE6D"
   },
   "source": [
    "Unsurprisingly, the result is not much different from what the random agent can achieve since our neural agent is initialized to a random policy.\n",
    "\n",
    "Let's train the agent for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzf_H7ThyE6E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the data in the prequisite section.\n",
    "if False:\n",
    "  from time import time\n",
    "  agent = NeuralAgent()\n",
    "\n",
    "  print(\"Training\")\n",
    "  agent.train()  # Tell the agent it should update its parameters.\n",
    "  starttime = time()\n",
    "  play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
    "\n",
    "  print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
    "\n",
    "  # Save the trained agent.\n",
    "  import os\n",
    "  os.makedirs('checkpoints', exist_ok=True)\n",
    "  torch.save(agent, 'checkpoints/agent_trained_on_single_game.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdT8TB7ayE6F"
   },
   "source": [
    "#### Testing the agent trained on a single game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElRy94-tyE6F"
   },
   "outputs": [],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n",
    "agent.test()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Dense rewards game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAgebJpEyE6F"
   },
   "source": [
    "Of course, since we trained on that single simple game, it's not surprinsing the agent can achieve a high score on it. It would be more interesting to evaluate the generalization capability of the agent.\n",
    "\n",
    "To do so, we are going to test the agent on another game drawn from the same game distribution (i.e. same world but the goal is to pick another food item). Let's generate `games/another_game.z8` with the same rewards density (`--rewards dense`) and the same goal description (`--goal detailed`), but using `--seed 1` and without the `--test` flag (to make sure the game is not part of the test set since `games/rewardsDense_goalDetailed.z8` is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPKY9jkMyE6G"
   },
   "outputs": [],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/tw-another_game.z8 -v -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOILdKH_yE6G"
   },
   "outputs": [],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "play(RandomAgent(), \"./games/tw-another_game.z8\")\n",
    "play(agent, \"./games/tw-another_game.z8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mMvSMb-yE6G"
   },
   "source": [
    "As we can see the trained agent does a bit better than the random agent. In order to improve the agent's generalization capability, we should train it on many different games drawn from the game distribution.\n",
    "\n",
    "One could use the following command to easily generate 100 training games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1xWlfjdyE6H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the data in the prequisite section.\n",
    "\n",
    "! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --format z8 --output training_games/ --seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He0cUnpuyE6H"
   },
   "source": [
    "Then, we train our agent on that set of training games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVLj-YKMyE6H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the data in the prequisite section.\n",
    "if False:\n",
    "  from time import time\n",
    "  agent = NeuralAgent()\n",
    "\n",
    "  print(\"Training on 100 games\")\n",
    "  agent.train()  # Tell the agent it should update its parameters.\n",
    "  starttime = time()\n",
    "  play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
    "  print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
    "\n",
    "  # Save the trained agent.\n",
    "  import os\n",
    "  os.makedirs('checkpoints', exist_ok=True)\n",
    "  torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_iOg8QpyE6H"
   },
   "source": [
    "#### Testing the agent trained on 100 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDn1WlTIyE6I"
   },
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n",
    "agent.test()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
    "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVVsVTyLyE6I"
   },
   "source": [
    "Compare it to the agent trained on a single game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UCCW3cvyE6I",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n",
    "agent.test()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
    "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B31IqhZAyE6J"
   },
   "source": [
    "#### Evaluating the agent on a test distribution\n",
    "We will generate 20 test games and evaluate the agent on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZevzwZbyE6J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the games in the prequisite section.\n",
    "\n",
    "! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --format z8 --output testing_games/ --seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHrQ8zjzyE6J"
   },
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n",
    "agent.test()\n",
    "play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)\n",
    "play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsyMnRm4yE6K"
   },
   "source": [
    "While not being perfect, the agent manage to score more points on average compared to the random agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdHfSBZ4yE6K"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Here are a few possible directions one can take to improve the agent's performance.\n",
    "- Adding more training games\n",
    "- Changing the agent architecture\n",
    "- Leveraging already trained word embeddings\n",
    "- Playing more games at once (see [`textworld.gym.make_batch`](https://textworld.readthedocs.io/en/latest/textworld.gym.html#textworld.gym.utils.make_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NepDIonbyE6K"
   },
   "source": [
    "\n",
    "## Papers about RL applied to text-based games\n",
    "* [Language Understanding for Text-based games using Deep Reinforcement Learning][narasimhan_et_al_2015]\n",
    "* [Learning How Not to Act in Text-based Games][haroush_et_al_2017]\n",
    "* [Deep Reinforcement Learning with a Natural Language Action Space][he_et_al_2015]\n",
    "* [What can you do with a rock? Affordance extraction via word embeddings][fulda_et_al_2017]\n",
    "* [Text-based adventures of the Golovin AI Agent][kostka_et_al_2017]\n",
    "* [Using reinforcement learning to learn how to play text-based games][zelinka_2018]\n",
    "\n",
    "[narasimhan_et_al_2015]: https://arxiv.org/abs/1506.08941\n",
    "[haroush_et_al_2017]: https://openreview.net/pdf?id=B1-tVX1Pz\n",
    "[he_et_al_2015]: https://arxiv.org/abs/1511.04636\n",
    "[fulda_et_al_2017]: https://arxiv.org/abs/1703.03429\n",
    "[kostka_et_al_2017]: https://arxiv.org/abs/1705.05637\n",
    "[zelinka_2018]: https://arxiv.org/abs/1801.01999"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
