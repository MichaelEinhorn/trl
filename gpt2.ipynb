{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPUzqhylyE5n"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MarcCote/TextWorld/blob/msr_summit_2021/notebooks/Building%20a%20simple%20agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJMSoUNwyE5r"
   },
   "source": [
    "# Building a simple agent with TextWorld\n",
    "\n",
    "This tutorial outlines the steps to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1661204255945,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "vqhSsad9Ekik",
    "outputId": "699529f6-2444-4ac3-a770-158ee6cc785d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "IS_COLAB = False\n",
    "try:\n",
    "    import google.colab, torch, sys\n",
    "    print(torch.__version__)\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Change runtime type to include a GPU.\")\n",
    "    IS_COLAB = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd drive/My Drive\n",
    "    %cd trl\n",
    "    \n",
    "print(IS_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnNgYvt6v0JI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icZs6NL4yE5s"
   },
   "source": [
    "### Prerequisite\n",
    "Install TextWorld as described in the [README.md](https://github.com/microsoft/TextWorld#readme). Most of the time, a simple `pip install` should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3494,
     "status": "ok",
     "timestamp": 1661204259434,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "EWTvsDGGyE5t",
    "outputId": "5753a9db-4b1c-4c35-c667-54ccd066d845"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !pip install textworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1661204265856,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "i6ZX-nVT_f3o",
    "outputId": "6e72a3ad-7a4e-4588-d410-1502592039ca"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTJ5BPyMyE5v"
   },
   "source": [
    "and [PyTorch](https://pytorch.org/) (tested with both v1.8.2 and v.1.9.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1661204266461,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "Z-hFU5D6yE5w",
    "outputId": "8b7dd308-b64c-43a1-ce0e-5fc03ad109b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d_K9f1byE5x"
   },
   "source": [
    "**[Optional]** Download all data beforehand. Otherwise, they are going to be generated as needed (slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/trl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10286,
     "status": "ok",
     "timestamp": 1661204276740,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "MiQnT0WayE5z",
    "outputId": "b546ab23-7b14-47d4-b56d-e486eff4b19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-25 08:05:52--  https://aka.ms/textworld/notebooks/data.zip\n",
      "Resolving aka.ms (aka.ms)... 104.87.239.185\n",
      "Connecting to aka.ms (aka.ms)|104.87.239.185|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://textworld.blob.core.windows.net/$web/notebooks/data.zip [following]\n",
      "--2022-08-25 08:05:52--  https://textworld.blob.core.windows.net/$web/notebooks/data.zip\n",
      "Resolving textworld.blob.core.windows.net (textworld.blob.core.windows.net)... 52.239.172.164\n",
      "Connecting to textworld.blob.core.windows.net (textworld.blob.core.windows.net)|52.239.172.164|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26926729 (26M) [application/octet-stream]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  25.68M  27.1MB/s    in 0.9s    \n",
      "\n",
      "2022-08-25 08:05:53 (27.1 MB/s) - ‘data.zip’ saved [26926729/26926729]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://aka.ms/textworld/notebooks/data.zip\n",
    "!unzip -nq data.zip && rm -f data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is8jBmJ4yE50"
   },
   "source": [
    "## Learning challenges\n",
    "Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n",
    "\n",
    "1. a combinatorial action space (that grows w.r.t. vocabulary)\n",
    "2. a really sparse reward signal.\n",
    "\n",
    "To ease the learning process, we will be requesting additional information alongside the game's narrative (as covered in [Playing TextWorld generated games with OpenAI Gym](Playing%20TextWorld%20generated%20games%20with%20OpenAI%20Gym.ipynb#Interact-with-the-game)). More specifically, we will request the following information:\n",
    "\n",
    "- __Description__:\n",
    "For every game state, we will get the output of the `look` command which describes the current location;\n",
    "\n",
    "- __Inventory__:\n",
    "For every game state, we will get the output of the `inventory` command which describes the player's inventory;\n",
    "\n",
    "- __Admissible commands__:\n",
    "For every game state, we will get the list of commands guaranteed to be understood by the game interpreter;\n",
    "\n",
    "- __Intermediate reward__:\n",
    "For every game state, we will get an intermediate reward which can either be:\n",
    "  - __-1__: last action needs to be undone before resuming the quest\n",
    "  -  __0__: last action didn't affect the quest\n",
    "  -  __1__: last action brought us closer to completing the quest\n",
    "\n",
    "- __Entities__:\n",
    "For every game, we will get a list of entity names that the agent can interact with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3GN-suFyE53"
   },
   "source": [
    "## Simple test games\n",
    "We can use TextWorld to generate a few simple games with the following handcrafted world\n",
    "```\n",
    "                     Bathroom\n",
    "                        +\n",
    "                        |\n",
    "                        +\n",
    "    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n",
    "      (P)               +                  +\n",
    "                        |                  |\n",
    "                        +                  +\n",
    "                   Living Room           Garden\n",
    "```\n",
    "where the goal is always to retrieve a hidden food item and put it on the stove which is located in the kitchen. One can lose the game if it eats the food item instead of putting it on the stove!\n",
    "\n",
    "Using `tw-make tw-simple ...`, we are going to generate the following 7 games:\n",
    "\n",
    "| gamefile | description |\n",
    "| :------- | :---------- |\n",
    "| `games/rewardsDense_goalDetailed.z8` | dense reward + detailed instructions |\n",
    "| `games/rewardsBalanced_goalDetailed.z8` | balanced rewards + detailed instructions |\n",
    "| `games/rewardsSparse_goalDetailed.z8` | sparse rewards + detailed instructions |\n",
    "| | |\n",
    "| `games/rewardsDense_goalBrief.z8` | dense rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsBalanced_goalBrief.z8` | balanced rewards + no instructions but the goal is mentionned |\n",
    "| `games/rewardsSparse_goalBrief.z8` | sparse rewards + no instructions but the goal is mentionned |\n",
    "| | |\n",
    "| `games/rewardsSparse_goalNone.z8` | sparse rewards + no instructions/goal<br>_Hint: there's an hidden note in the game that describes the goal!_ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1661204295821,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "2fXqhZzMyE55"
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the games in the prequisite section.\n",
    "if False:\n",
    "  # Same as !make_games.sh\n",
    "  !tw-make tw-simple --rewards dense    --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8\n",
    "  !tw-make tw-simple --rewards balanced --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalDetailed.z8\n",
    "  !tw-make tw-simple --rewards sparse   --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalDetailed.z8\n",
    "  !tw-make tw-simple --rewards dense    --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsDense_goalBrief.z8\n",
    "  !tw-make tw-simple --rewards balanced --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsBalanced_goalBrief.z8\n",
    "  !tw-make tw-simple --rewards sparse   --goal brief    --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8\n",
    "  !tw-make tw-simple --rewards sparse   --goal none     --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCFwvj-_yE57"
   },
   "source": [
    "## Building the random baseline\n",
    "Let's start with building an agent that simply selects an admissible command at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0LWqo9p4zjh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1661204299711,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "IubeCZ8AyE57"
   },
   "outputs": [],
   "source": [
    "from typing import Mapping, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld.gym\n",
    "\n",
    "\n",
    "class RandomAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "    \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        return self.rng.choice(infos[\"admissible_commands\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661204302353,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "mU6zr6hkApHX"
   },
   "outputs": [],
   "source": [
    "class HumanAgent(textworld.gym.Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> textworld.EnvInfos:\n",
    "        return textworld.EnvInfos(admissible_commands=True)\n",
    "    \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        print(obs)\n",
    "        return input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HvWUj1myE59"
   },
   "source": [
    "## Play function\n",
    "Let's write a simple play function that we will use to evaluate our agent on a given game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1661204304780,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "MXggmJpDyE59"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import gym\n",
    "import textworld.gym\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def play(agent, path, max_step=100, nb_episodes=10, verbose=True):\n",
    "    torch.manual_seed(20211021)  # For reproducibility when using action sampling.\n",
    "\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "    \n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
    "        \n",
    "    print(gamefiles)\n",
    "    env_id = textworld.gym.register_games(gamefiles,\n",
    "                                          request_infos=infos_to_request,\n",
    "                                          max_episode_steps=max_step)\n",
    "    print(env_id)\n",
    "    env = gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path), end=\"\")\n",
    "        else:\n",
    "            print(os.path.basename(path), end=\"\")\n",
    "        \n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores, avg_norm_scores = [], [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        obs, infos = env.reset()  # Start new episode.\n",
    "\n",
    "        score = 0\n",
    "        done = False\n",
    "        nb_moves = 0\n",
    "        while not done:\n",
    "            command = agent.act(obs, score, done, infos)\n",
    "            obs, score, done, infos = env.step(command)\n",
    "            if hasattr(agent, 'reportScore'):\n",
    "                agent.reportScore(score, done, infos)\n",
    "            nb_moves += 1\n",
    "        \n",
    "        agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "                \n",
    "        if verbose:\n",
    "            print(\".\", end=\"\")\n",
    "        avg_moves.append(nb_moves)\n",
    "        avg_scores.append(score)\n",
    "        avg_norm_scores.append(score / infos[\"max_score\"])\n",
    "\n",
    "    env.close()\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))\n",
    "        else:\n",
    "            msg = \"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}.\"\n",
    "            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos[\"max_score\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgZp0TyOyE5-"
   },
   "source": [
    "#### Evaluate the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1071,
     "status": "ok",
     "timestamp": 1661204309531,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "ID0GlEDwd6V-",
    "outputId": "79d46614-d0b2-492a-8ace-85a1e9041df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md  \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  requirements.txt    \u001b[01;34mtesting_games\u001b[0m/\n",
      "LICENSE          \u001b[01;34mdocs\u001b[0m/         settings.ini        \u001b[01;34mtraining_games\u001b[0m/\n",
      "MANIFEST.in      \u001b[01;34mgames\u001b[0m/        setup.py            \u001b[01;34mtrl\u001b[0m/\n",
      "Makefile         gpt2.ipynb    test_trl.py\n",
      "README.md        \u001b[01;34mnbs\u001b[0m/          test_trl_gptneo.py\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1661204311811,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "DqbOAitveJzD",
    "outputId": "975bb47e-18d7-401f-e72f-7560fb7cb5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"./games/tw-rewardsDense_goalDetailed.z8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "error",
     "timestamp": 1661204314795,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "c7eTIGDHyE5-",
    "outputId": "f62468ae-e9f7-4709-e786-f809bd9cfffa"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # We report the score and steps averaged over 10 playthroughs.\n",
    "    play(RandomAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")    # Dense rewards\n",
    "    play(RandomAgent(), \"./games/tw-rewardsBalanced_goalDetailed.z8\") # Balanced rewards\n",
    "    play(RandomAgent(), \"./games/tw-rewardsSparse_goalDetailed.z8\")   # Sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "error",
     "timestamp": 1661204187375,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "hwOUgTGoAm4u",
    "outputId": "6a726c0b-431e-4cbd-e78e-439e0d8c8b74"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    play(HumanAgent(), \"./games/tw-rewardsDense_goalDetailed.z8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QR5htLYyE6A"
   },
   "source": [
    "## Neural agent\n",
    "\n",
    "Now, let's create an agent that can learn to play text-based games. The agent will be trained to select a command from the list of admissible commands given the current game's narrative, inventory, and room description. Here is an overview of the architecture used for the agent: \n",
    "\n",
    "<div>\n",
    "  <img src=\"https://raw.githubusercontent.com/MarcCote/TextWorld/msr_summit_2021/notebooks/figs/neural_agent.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWMuW7wKyE6A"
   },
   "source": [
    "### Code\n",
    "Here's the implementation of that learning agent built with [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1661203662391,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "iuwfllu5yE6B",
    "outputId": "df1ebc49-3fa5-4b86-c3fd-1cc82f872133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:118: DeprecationWarning: invalid escape sequence \\-\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Mapping, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textworld\n",
    "import textworld.gym\n",
    "from textworld import EnvInfos\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class CommandScorer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CommandScorer, self).__init__()\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        self.embedding    = nn.Embedding(input_size, hidden_size)\n",
    "        self.encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.cmd_encoder_gru  = nn.GRU(hidden_size, hidden_size)\n",
    "        self.state_gru    = nn.GRU(hidden_size, hidden_size)\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.state_hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        self.critic       = nn.Linear(hidden_size, 1)\n",
    "        self.att_cmd      = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, obs, commands, **kwargs):\n",
    "        input_length = obs.size(0)\n",
    "        batch_size = obs.size(1)\n",
    "        nb_cmds = commands.size(1)\n",
    "\n",
    "        embedded = self.embedding(obs)\n",
    "        encoder_output, encoder_hidden = self.encoder_gru(embedded)\n",
    "        state_output, state_hidden = self.state_gru(encoder_hidden, self.state_hidden)\n",
    "        self.state_hidden = state_hidden\n",
    "        value = self.critic(state_output)\n",
    "\n",
    "        # Attention network over the commands.\n",
    "        cmds_embedding = self.embedding.forward(commands)\n",
    "        _, cmds_encoding_last_states = self.cmd_encoder_gru.forward(cmds_embedding)  # 1 x cmds x hidden\n",
    "\n",
    "        # Same observed state for all commands.\n",
    "        cmd_selector_input = torch.stack([state_hidden] * nb_cmds, 2)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Same command choices for the whole batch.\n",
    "        cmds_encoding_last_states = torch.stack([cmds_encoding_last_states] * batch_size, 1)  # 1 x batch x cmds x hidden\n",
    "\n",
    "        # Concatenate the observed state and command encodings.\n",
    "        cmd_selector_input = torch.cat([cmd_selector_input, cmds_encoding_last_states], dim=-1)\n",
    "\n",
    "        # Compute one score per command.\n",
    "        scores = F.relu(self.att_cmd(cmd_selector_input)).squeeze(-1)  # 1 x Batch x cmds\n",
    "\n",
    "        probs = F.softmax(scores, dim=2)  # 1 x Batch x cmds\n",
    "        index = probs[0].multinomial(num_samples=1).unsqueeze(0) # 1 x batch x indx\n",
    "        return scores, index, value\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.state_hidden = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class NeuralAgent:\n",
    "    \"\"\" Simple Neural Agent for playing TextWorld games. \"\"\"\n",
    "    MAX_VOCAB_SIZE = 1000\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 1000\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "        self.id2word = [\"<PAD>\", \"<UNK>\"]\n",
    "        self.word2id = {w: i for i, w in enumerate(self.id2word)}\n",
    "        \n",
    "        self.model = CommandScorer(input_size=self.MAX_VOCAB_SIZE, hidden_size=128)\n",
    "        if device == \"cuda\":\n",
    "          self.model = self.model.cuda()\n",
    "          \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.00003)\n",
    "        \n",
    "        self.mode = \"test\"\n",
    "    \n",
    "    def train(self):\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = \"test\"\n",
    "        self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        won=True, lost=True)\n",
    "    \n",
    "    def _get_word_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            if len(self.word2id) >= self.MAX_VOCAB_SIZE:\n",
    "                return self.word2id[\"<UNK>\"]\n",
    "            \n",
    "            self.id2word.append(word)\n",
    "            self.word2id[word] = len(self.word2id)\n",
    "            \n",
    "        return self.word2id[word]\n",
    "            \n",
    "    def _tokenize(self, text):\n",
    "        # Simple tokenizer: strip out all non-alphabetic characters.\n",
    "        text = re.sub(\"[^a-zA-Z0-9\\- ]\", \" \", text)\n",
    "        word_ids = list(map(self._get_word_id, text.split()))\n",
    "        return word_ids\n",
    "\n",
    "    def _process(self, texts):\n",
    "        texts = list(map(self._tokenize, texts))\n",
    "        max_len = max(len(l) for l in texts)\n",
    "        padded = np.ones((len(texts), max_len)) * self.word2id[\"<PAD>\"]\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            padded[i, :len(text)] = text\n",
    "\n",
    "        padded_tensor = torch.from_numpy(padded).type(torch.long).to(device)\n",
    "        padded_tensor = padded_tensor.permute(1, 0) # Batch x Seq => Seq x Batch\n",
    "        return padded_tensor\n",
    "      \n",
    "    def _discount_rewards(self, last_values):\n",
    "        returns, advantages = [], []\n",
    "        R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        input_ = \"{}\\n{}\\n{}\".format(obs, infos[\"description\"], infos[\"inventory\"])\n",
    "        \n",
    "        # Tokenize and pad the input and the commands to chose from.\n",
    "        input_tensor = self._process([input_]).to(device)\n",
    "        commands_tensor = self._process(infos[\"admissible_commands\"]).to(device)\n",
    "        \n",
    "        # Get our next action and value prediction.\n",
    "        outputs, indexes, values = self.model(input_tensor, commands_tensor)\n",
    "        action = infos[\"admissible_commands\"][indexes[0]]\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            if done:\n",
    "                self.model.reset_hidden(1)\n",
    "            return action\n",
    "        \n",
    "        self.no_train_step += 1\n",
    "        \n",
    "        if self.transitions:\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "                \n",
    "            self.transitions[-1][0] = reward  # Update reward information.\n",
    "        \n",
    "        self.stats[\"max\"][\"score\"].append(score)\n",
    "        if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "            # Update model\n",
    "            returns, advantages = self._discount_rewards(values)\n",
    "            \n",
    "            loss = 0\n",
    "            for transition, ret, advantage in zip(self.transitions, returns, advantages):\n",
    "                reward, indexes_, outputs_, values_ = transition\n",
    "                \n",
    "                advantage        = advantage.detach() # Block gradients flow here.\n",
    "                probs            = F.softmax(outputs_, dim=2)\n",
    "                log_probs        = torch.log(probs)\n",
    "                log_action_probs = log_probs.gather(2, indexes_)\n",
    "                policy_loss      = (-log_action_probs * advantage).sum()\n",
    "                value_loss       = (.5 * (values_ - ret) ** 2.).sum()\n",
    "                entropy     = (-probs * log_probs).sum()\n",
    "                loss += policy_loss + 0.5 * value_loss - 0.1 * entropy\n",
    "                \n",
    "                self.stats[\"mean\"][\"reward\"].append(reward)\n",
    "                self.stats[\"mean\"][\"policy\"].append(policy_loss.item())\n",
    "                self.stats[\"mean\"][\"value\"].append(value_loss.item())\n",
    "                self.stats[\"mean\"][\"entropy\"].append(entropy.item())\n",
    "                self.stats[\"mean\"][\"confidence\"].append(torch.exp(log_action_probs).item())\n",
    "            \n",
    "            if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                msg = \"{:6d}. \".format(self.no_train_step)\n",
    "                msg += \"  \".join(\"{}: {: 3.3f}\".format(k, np.mean(v)) for k, v in self.stats[\"mean\"].items())\n",
    "                msg += \"  \" + \"  \".join(\"{}: {:2d}\".format(k, np.max(v)) for k, v in self.stats[\"max\"].items())\n",
    "                msg += \"  vocab: {:3d}\".format(len(self.id2word))\n",
    "                print(msg)\n",
    "                self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            self.transitions = []\n",
    "            self.model.reset_hidden(1)\n",
    "        else:\n",
    "            # Keep information about transitions for Truncated Backpropagation Through Time.\n",
    "            self.transitions.append([None, indexes, outputs, values])  # Reward will be set on the next call\n",
    "        \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1661203662392,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "Jb5D6J67xwu_"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class buffer:\n",
    "    def __init__(self, max_size):\n",
    "      self.max_size = max_size\n",
    "      self.queue = deque([])\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.queue)\n",
    "\n",
    "    def __iter__(self):\n",
    "      return iter(self.queue)\n",
    "\n",
    "    def append(self, obj):\n",
    "      self.queue.append(obj)\n",
    "      if len(self.queue) > self.max_size:\n",
    "        return self.queue.pop()\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    def clear(self):\n",
    "      self.queue = deque([])\n",
    "\n",
    "# Utilities for dealing with tokens\n",
    "def make_inputs(tokenizer, prompts, device=\"cuda\"):\n",
    "    token_lists = [tokenizer.encode(p) for p in prompts]\n",
    "    maxlen = max(len(t) for t in token_lists)\n",
    "    if \"[PAD]\" in tokenizer.all_special_tokens:\n",
    "        pad_id = tokenizer.all_special_ids[tokenizer.all_special_tokens.index(\"[PAD]\")]\n",
    "    else:\n",
    "        pad_id = 0\n",
    "    input_ids = [[pad_id] * (maxlen - len(t)) + t for t in token_lists]\n",
    "    # position_ids = [[0] * (maxlen - len(t)) + list(range(len(t))) for t in token_lists]\n",
    "    attention_mask = [[0] * (maxlen - len(t)) + [1] * len(t) for t in token_lists]\n",
    "    return dict(\n",
    "        input_ids=torch.tensor(input_ids).to(device),\n",
    "    #    position_ids=torch.tensor(position_ids).to(device),\n",
    "        attention_mask=torch.tensor(attention_mask).to(device),\n",
    "    )\n",
    "\n",
    "def predict_token(mt, prompts, return_p=False):\n",
    "    inp = make_inputs(mt.tokenizer, prompts)\n",
    "    preds, p = predict_from_input(mt.model, inp)\n",
    "    result = [mt.tokenizer.decode(c) for c in preds]\n",
    "    if return_p:\n",
    "        result = (result, p)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_from_input(model, inp):\n",
    "    out = model(**inp)[\"logits\"]\n",
    "    probs = torch.softmax(out[:, -1], dim=1)\n",
    "    p, preds = torch.max(probs, dim=1)\n",
    "    return preds, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1661203663033,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "98LDtQg6CjNO"
   },
   "outputs": [],
   "source": [
    "from transformers import top_k_top_p_filtering\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NLPAgent:\n",
    "    \"\"\" Hugging Face Transformer Agent \"\"\"\n",
    "    UPDATE_FREQUENCY = 10\n",
    "    LOG_FREQUENCY = 10\n",
    "    GAMMA = 0.9\n",
    "    MEMORY_LEN = 3\n",
    "    \n",
    "    def __init__(self, model, model_ref, tokenizer, humanTurns=0) -> None:\n",
    "        self._initialized = False\n",
    "        self._epsiode_has_started = False\n",
    "       \n",
    "        self.memory = buffer(self.MEMORY_LEN)\n",
    "\n",
    "        self.model = model\n",
    "        self.model_ref = model_ref\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.humanTurns = humanTurns\n",
    "        self.humanTurnsRem = self.humanTurns\n",
    "        \n",
    "        if model_ref is not None:\n",
    "          # initialize trainer\n",
    "          ppo_config = {'batch_size': self.UPDATE_FREQUENCY, 'forward_batch_size': 1}\n",
    "          self.ppo_trainer = PPOTrainer(self.model, self.model_ref, self.tokenizer, **ppo_config)\n",
    "          self.valueHead = self.ppo_trainer.valueHead\n",
    "\n",
    "        if device == \"cuda\":\n",
    "          self.model.cuda()\n",
    "          self.tokenizer.cuda()\n",
    "          \n",
    "        self.mode = \"test\"\n",
    "\n",
    "        self.clearTextWorldArt = True\n",
    "    \n",
    "    def train(self):\n",
    "        if self.model_ref is None:\n",
    "          raise NotImplementedError\n",
    "        self.mode = \"train\"\n",
    "        self.stats = {\"max\": defaultdict(list), \"mean\": defaultdict(list)}\n",
    "        self.transitions = []\n",
    "        # self.model.reset_hidden(1)\n",
    "        self.last_score = 0\n",
    "        self.no_train_step = 0\n",
    "        self.clearTextWorldArt = True\n",
    "        \n",
    "        self.memory.clear()\n",
    "    \n",
    "    def test(self):\n",
    "        self.clearTextWorldArt = True\n",
    "        self.mode = \"test\"\n",
    "        # self.model.reset_hidden(1)\n",
    "        \n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos(description=True, inventory=True, admissible_commands=True,\n",
    "                        won=True, lost=True)\n",
    "      \n",
    "    def _discount_rewards(self, last_values=None):\n",
    "        returns, advantages = [], []\n",
    "        if last_values is None:\n",
    "            # not sure if this makes sense for when there is no next state\n",
    "            _, _, _, R = self.transitions[-1]\n",
    "        else:\n",
    "            R = last_values.data\n",
    "        for t in reversed(range(len(self.transitions))):\n",
    "            rewards, _, _, values = self.transitions[t]\n",
    "            R = rewards + self.GAMMA * R\n",
    "            adv = R - values\n",
    "            returns.append(R)\n",
    "            advantages.append(adv)\n",
    "            \n",
    "        return returns[::-1], advantages[::-1]\n",
    "    \n",
    "    # fill in results from action and train if time\n",
    "    def reportScore(self, score, done, infos):\n",
    "        if self.mode == \"train\":\n",
    "            reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "            self.last_score = score\n",
    "            if infos[\"won\"]:\n",
    "                reward += 100\n",
    "            if infos[\"lost\"]:\n",
    "                reward -= 100\n",
    "\n",
    "            self.transitions[-1][0] = reward  # Update reward information. Was initialized as none\n",
    "\n",
    "            self.no_train_step += 1\n",
    "\n",
    "            self.stats[\"max\"][\"score\"].append(score)\n",
    "            if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "                # get discounted returns and advantages across multiple actions. Currently not used\n",
    "                returns, advantages = self._discount_rewards()\n",
    "\n",
    "                query = []\n",
    "                response = []\n",
    "                rewardList = []\n",
    "\n",
    "                for t in reversed(range(len(self.transitions))):\n",
    "                  rew, prompt, action, values = self.transitions[t]\n",
    "\n",
    "                  query.append(prompt[0])\n",
    "                  response.append(action[0])\n",
    "                  rewardList.append(rew)\n",
    "\n",
    "                train_stats = self.ppo_trainer.step(query, response, rewardList)\n",
    "\n",
    "                if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "                  print(train_stats)\n",
    "\n",
    "                self.transitions = []\n",
    "        \n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> Optional[str]:\n",
    "        if self.clearTextWorldArt:\n",
    "          self.clearTextWorldArt = False\n",
    "          if \"Welcome to TextWorld!\" in obs:\n",
    "              obs = obs[obs.index(\"Welcome to TextWorld!\"):]\n",
    "          elif \"$$$$$$$\" in obs:\n",
    "              obs = obs[obs.rindex(\"$$$$$$$\"):]\n",
    "        \n",
    "        # Build agent's observation: feedback + look + inventory.\n",
    "        pastStates = \"\"\n",
    "        for mem in self.memory:\n",
    "          pastStates = pastStates + mem + \"\\n\"\n",
    "        admissible_commands_str = \"options: \"\n",
    "        for adm_cmd in infos[\"admissible_commands\"]:\n",
    "            admissible_commands_str += adm_cmd + \", \"\n",
    "        input_ = \"{}\\n{}\\n{}\\n{}\\nYou\".format(obs, infos[\"description\"], infos[\"inventory\"], admissible_commands_str)\n",
    "        prompt = pastStates + input_\n",
    "        \n",
    "        # grabs value of last token in action\n",
    "        values = 0\n",
    "        # convert text to tensor\n",
    "        input_ids = self.tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "        print(\"prompt tokens: \", input_ids.shape)\n",
    "        print(input_)\n",
    "\n",
    "        if self.humanTurnsRem > 0:\n",
    "          action = input()\n",
    "          self.memory.append(input_ + action)\n",
    "          self.humanTurnsRem -= 1\n",
    "          return action\n",
    "\n",
    "        new_tokens = 0\n",
    "        next_token = None\n",
    "        \n",
    "        cache = None\n",
    "        \n",
    "        while new_tokens == 0 or (new_tokens < 20 and \"\\n\" not in self.tokenizer.decode(next_token) \n",
    "                                  and next_token != self.tokenizer.eos_token):\n",
    "          # run model\n",
    "          with torch.no_grad():\n",
    "                # get logits, only get last value\n",
    "                if cache is None:\n",
    "                    lmOut = self.model(input_ids, output_hidden_states=True, use_cache=True)\n",
    "                else:\n",
    "                    lmOut = self.model(input_ids[:,-1:], output_hidden_states=True, use_cache=True, past_key_values=cache)\n",
    "                # print(dir(lmOut))\n",
    "                logits, hidden_state, cache = lmOut.logits, lmOut.hidden_states[-1], lmOut.past_key_values\n",
    "                \n",
    "                # hidden_state = hidden_state[-1]\n",
    "                values = self.valueHead(hidden_state)\n",
    "                \n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=0, top_p=1)\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "                \n",
    "                new_tokens += 1\n",
    "\n",
    "        del cache\n",
    "        del hidden_state\n",
    "        \n",
    "        action_tens = input_ids[:,-new_tokens:].to(\"cpu\")\n",
    "        prompt_tens = input_ids[:,:-new_tokens].to(\"cpu\")\n",
    "        \n",
    "        del input_ids\n",
    "\n",
    "        action = self.tokenizer.decode(action_tens[0,:])\n",
    "\n",
    "        print(\"action\")\n",
    "        print(action)\n",
    "        \n",
    "        # only grab last token\n",
    "        values = values[0,-1,0]\n",
    "        print(\"last token value in action\")\n",
    "        print(values)\n",
    "\n",
    "        self.memory.append(input_ + action)\n",
    "        \n",
    "        if self.mode == \"train\" and not done:\n",
    "            self.transitions.append([None, prompt_tens, action_tens, values])  # Reward will be set on the next call\n",
    "            \n",
    "        if done:\n",
    "            self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "            self.memory.clear()\n",
    "            self.clearTextWorldArt = True\n",
    "            self.humanTurnsRem = self.humanTurns\n",
    "            \n",
    "        return action\n",
    "        \n",
    "        # moved to report score\n",
    "#         if self.mode == \"test\":\n",
    "#             # if done:\n",
    "#             #     self.model.reset_hidden(1)\n",
    "#             return action\n",
    "        \n",
    "#         if self.transitions:\n",
    "#             reward = score - self.last_score  # Reward is the gain/loss in score.\n",
    "#             self.last_score = score\n",
    "#             if infos[\"won\"]:\n",
    "#                 reward += 100\n",
    "#             if infos[\"lost\"]:\n",
    "#                 reward -= 100\n",
    "                \n",
    "#             self.transitions[-1][0] = reward  # Update reward information. Was initialized as none\n",
    "        \n",
    "#         self.no_train_step += 1\n",
    "        \n",
    "#         self.stats[\"max\"][\"score\"].append(score)\n",
    "#         if self.no_train_step % self.UPDATE_FREQUENCY == 0:\n",
    "#             # get discounted returns and advantages across multiple actions. Currently not used\n",
    "#             returns, advantages = self._discount_rewards(values)\n",
    "\n",
    "#             query = []\n",
    "#             response = []\n",
    "#             rewardList = []\n",
    "\n",
    "#             for t in reversed(range(len(self.transitions))):\n",
    "#               rewards, prompt, action, values = self.transitions[t]\n",
    "\n",
    "#               query.append(prompt[0])\n",
    "#               response.append(action[0])\n",
    "#               rewardList.append(rewards)\n",
    "\n",
    "#             train_stats = self.ppo_trainer.step(query, response, rewardList)\n",
    "\n",
    "#             if self.no_train_step % self.LOG_FREQUENCY == 0:\n",
    "#               print(train_stats)\n",
    "            \n",
    "#             self.transitions = []\n",
    "            \n",
    "            \n",
    "#         else:\n",
    "#             # Keep information about transitions for ppo\n",
    "            \n",
    "#             self.transitions.append([None, prompt_tens, action_tens, values])  # Reward will be set on the next call\n",
    "        \n",
    "#         if done:\n",
    "#             self.last_score = 0  # Will be starting a new episode. Reset the last score.\n",
    "#             self.memory.clear()\n",
    "#             self.clearTextWorldArt = True\n",
    "#             self.humanTurnsRem = self.humanTurns\n",
    "        \n",
    "#         return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1661203668768,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "3PQ5YwDMI0Vi",
    "outputId": "110d4e2e-5014-4067-a019-25f23fd70bd8"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3883,
     "status": "ok",
     "timestamp": 1661203672647,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "NlEgcVAuIQWo",
    "outputId": "9be1d2f6-2de4-4d00-ed5d-7ced9aa2563e"
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15426,
     "status": "ok",
     "timestamp": 1661203688056,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "6hnyEtRaDJqg",
    "outputId": "8120b492-b50a-4a5b-d25a-204cd0fc2e22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/multiclass.py:13: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/usr/lib/python3/dist-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/lib/python3/dist-packages/pandas/util/testing.py:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import pandas._libs.testing as _testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "\n",
    "import importlib\n",
    "importlib.reload(trl)\n",
    "from trl.ppoValHead import PPOTrainer\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# get models\n",
    "# gpt2_model = GPT2HeadWithValueModel.from_pretrained('gpt2')\n",
    "# gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained('gpt2')\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "# model_name = 'gptj'\n",
    "\n",
    "low_ram = True\n",
    "\n",
    "# gpt2 and gpt2-xl\n",
    "if 'gpt2' in model_name:\n",
    "    from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model_ref = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "elif model_name == 'gptj':\n",
    "    from transformers import GPT2Tokenizer, GPTJForCausalLM\n",
    "    if low_ram:\n",
    "        model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "        model_ref = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "    else:\n",
    "        model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "        model_ref = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(model.config.torch_dtype)\n",
    "\n",
    "model = model.to(device)\n",
    "model_ref = model_ref.to(device)\n",
    "# tokenizer doesn't have a to device\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "# model_name = r\"gpt2-xl\"\n",
    "\n",
    "# Note that if you trace other models, you should set noise_level appropriately.\n",
    "# (We use 0.03 for gpt-neox-20b and 0.025 for gpt-j-6b)\n",
    "# model_name = r\"EleutherAI/gpt-neox-20b\"\n",
    "# model_name = r\"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# torch_dtype = torch.float16 if '20b' in model_name else None\n",
    "\n",
    "# mt = ModelAndTokenizer(model_name, low_cpu_mem_usage=IS_COLAB, torch_dtype=torch_dtype)\n",
    "# predict_token(mt, ['Megan Rapinoe plays the sport of',\n",
    "#                    'The Space Needle is in the city of'\n",
    "#                   ], return_p=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueHead(\n",
      "  (summary): Linear(in_features=1600, out_features=1, bias=True)\n",
      "  (activation): Identity()\n",
      "  (first_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (last_dropout): Identity()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "GPT2LMHeadModel                                    --\n",
       "├─GPT2Model: 1-1                                   --\n",
       "│    └─Embedding: 2-1                              80,411,200\n",
       "│    └─Embedding: 2-2                              1,638,400\n",
       "│    └─Dropout: 2-3                                --\n",
       "│    └─ModuleList: 2-4                             --\n",
       "│    │    └─GPT2Block: 3-1                         30,740,800\n",
       "│    │    └─GPT2Block: 3-2                         30,740,800\n",
       "│    │    └─GPT2Block: 3-3                         30,740,800\n",
       "│    │    └─GPT2Block: 3-4                         30,740,800\n",
       "│    │    └─GPT2Block: 3-5                         30,740,800\n",
       "│    │    └─GPT2Block: 3-6                         30,740,800\n",
       "│    │    └─GPT2Block: 3-7                         30,740,800\n",
       "│    │    └─GPT2Block: 3-8                         30,740,800\n",
       "│    │    └─GPT2Block: 3-9                         30,740,800\n",
       "│    │    └─GPT2Block: 3-10                        30,740,800\n",
       "│    │    └─GPT2Block: 3-11                        30,740,800\n",
       "│    │    └─GPT2Block: 3-12                        30,740,800\n",
       "│    │    └─GPT2Block: 3-13                        30,740,800\n",
       "│    │    └─GPT2Block: 3-14                        30,740,800\n",
       "│    │    └─GPT2Block: 3-15                        30,740,800\n",
       "│    │    └─GPT2Block: 3-16                        30,740,800\n",
       "│    │    └─GPT2Block: 3-17                        30,740,800\n",
       "│    │    └─GPT2Block: 3-18                        30,740,800\n",
       "│    │    └─GPT2Block: 3-19                        30,740,800\n",
       "│    │    └─GPT2Block: 3-20                        30,740,800\n",
       "│    │    └─GPT2Block: 3-21                        30,740,800\n",
       "│    │    └─GPT2Block: 3-22                        30,740,800\n",
       "│    │    └─GPT2Block: 3-23                        30,740,800\n",
       "│    │    └─GPT2Block: 3-24                        30,740,800\n",
       "│    │    └─GPT2Block: 3-25                        30,740,800\n",
       "│    │    └─GPT2Block: 3-26                        30,740,800\n",
       "│    │    └─GPT2Block: 3-27                        30,740,800\n",
       "│    │    └─GPT2Block: 3-28                        30,740,800\n",
       "│    │    └─GPT2Block: 3-29                        30,740,800\n",
       "│    │    └─GPT2Block: 3-30                        30,740,800\n",
       "│    │    └─GPT2Block: 3-31                        30,740,800\n",
       "│    │    └─GPT2Block: 3-32                        30,740,800\n",
       "│    │    └─GPT2Block: 3-33                        30,740,800\n",
       "│    │    └─GPT2Block: 3-34                        30,740,800\n",
       "│    │    └─GPT2Block: 3-35                        30,740,800\n",
       "│    │    └─GPT2Block: 3-36                        30,740,800\n",
       "│    │    └─GPT2Block: 3-37                        30,740,800\n",
       "│    │    └─GPT2Block: 3-38                        30,740,800\n",
       "│    │    └─GPT2Block: 3-39                        30,740,800\n",
       "│    │    └─GPT2Block: 3-40                        30,740,800\n",
       "│    │    └─GPT2Block: 3-41                        30,740,800\n",
       "│    │    └─GPT2Block: 3-42                        30,740,800\n",
       "│    │    └─GPT2Block: 3-43                        30,740,800\n",
       "│    │    └─GPT2Block: 3-44                        30,740,800\n",
       "│    │    └─GPT2Block: 3-45                        30,740,800\n",
       "│    │    └─GPT2Block: 3-46                        30,740,800\n",
       "│    │    └─GPT2Block: 3-47                        30,740,800\n",
       "│    │    └─GPT2Block: 3-48                        30,740,800\n",
       "│    └─LayerNorm: 2-5                              3,200\n",
       "├─Linear: 1-2                                      80,411,200\n",
       "===========================================================================\n",
       "Total params: 1,557,611,200\n",
       "Trainable params: 1,557,611,200\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = NLPAgent(model, model_ref, tokenizer, humanTurns=0)\n",
    "print(agent.ppo_trainer.valueHead)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkN6y-r2yE6C"
   },
   "source": [
    "### Training the neural agent\n",
    "Let's first evaluate the agent before training to get a sense of its initial performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "error",
     "timestamp": 1661203726351,
     "user": {
      "displayName": "Michael Einhorn",
      "userId": "04443615996412342483"
     },
     "user_tz": 420
    },
    "id": "_HyFeWb8yE6D",
    "outputId": "4389785f-ffe0-40a4-c370-9387517b982b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./games/tw-rewardsDense_goalDetailed.z8']\n",
      "tw-v0\n",
      "tw-rewardsDense_goalDetailed.z8prompt tokens:  torch.Size([1, 386])\n",
      "Welcome to TextWorld! Here is your task for today. First of all, you could, like, open the chest drawer. Then, pick up the old key from the chest drawer. After picking up the old key, assure that the wooden door is unlocked. Then, ensure that the wooden door is open. After pulling open the wooden door, take a trip east. And then, make sure that the screen door is wide open. After that, make an effort to move east. With that done, attempt to take a trip south. Then, pick-up the carrot from the floor of the garden. And then, attempt to go to the north. If you can succeed at that, make an effort to take a trip west. And then, rest the carrot on the stove. Got that? Good!\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.0695, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 547])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.3299, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 708])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.8413, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.4989, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.2638, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.5892, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pulled enough dirt off the trunk and opened it again. A fishing pole and two thistles.\n",
      "last token value in action\n",
      "tensor(0.4808, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "That's not a verb I recognise.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.6548, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.3345, device='cuda:0')\n",
      "prompt tokens:  torch.Size([1, 869])\n",
      "You can't see any such thing.\n",
      "\n",
      "-= Bedroom =-\n",
      "You are in a bedroom. A standard kind of place.\n",
      "\n",
      "You can make out a chest drawer. There's something strange about this being here, but you can't put your finger on it. You see a closed antique trunk close by. Look over there! a king-size bed. The king-size bed appears to be empty.\n",
      "\n",
      "There is a closed wooden door leading east.\n",
      "You are carrying nothing.\n",
      "options: examine antique trunk, examine chest drawer, examine king-size bed, examine wooden door, inventory, look, open antique trunk, open chest drawer, \n",
      "You\n",
      "action\n",
      " pull the wood and soil from the trunk and open it up. A flower pot and a picket\n",
      "last token value in action\n",
      "tensor(1.6916, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/trl/trl/core.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if isinstance(v, collections.Mapping):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 47.54 GiB total capacity; 44.94 GiB already allocated; 84.69 MiB free; 46.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0d3ee418245d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./games/tw-rewardsDense_goalDetailed.z8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-bd81980324e5>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(agent, path, max_step, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reportScore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreportScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mnb_moves\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-df3563fcdb9f>\u001b[0m in \u001b[0;36mreportScore\u001b[0;34m(self, score, done, infos)\u001b[0m\n\u001b[1;32m    106\u001b[0m                   \u001b[0mrewardList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewardList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_train_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trl/trl/ppoValHead.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, queries, responses, scores)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 train_stats = self.train_minibatch(logprobs[idx].unsqueeze(0), values[idx].unsqueeze(0),\n\u001b[0m\u001b[1;32m    175\u001b[0m                                                    \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                                                    \u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trl/trl/ppoValHead.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, logprobs, values, rewards, query, response, model_input)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;34m\"\"\"Train one PPO minibatch\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mloss_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_stats\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_p\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trl/trl/ppoValHead.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, old_logprobs, values, rewards, query, response, model_input)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vf_coef'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvf_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_from_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mapproxkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_logprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mpolicykl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_logprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trl/trl/core.py\u001b[0m in \u001b[0;36mentropy_from_logits\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m\"\"\"Calculate entropy from logits.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 47.54 GiB total capacity; 44.94 GiB already allocated; 84.69 MiB free; 46.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "agent.train()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "print(\"Training on 100 games\")\n",
    "agent.train()  # Tell the agent it should update its parameters.\n",
    "starttime = time()\n",
    "play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
    "print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
    "\n",
    "# Save the trained agent.\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POiGO-PmyE6D"
   },
   "source": [
    "Unsurprisingly, the result is not much different from what the random agent can achieve since our neural agent is initialized to a random policy.\n",
    "\n",
    "Let's train the agent for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzf_H7ThyE6E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the data in the prequisite section.\n",
    "if False:\n",
    "  from time import time\n",
    "  agent = NeuralAgent()\n",
    "\n",
    "  print(\"Training\")\n",
    "  agent.train()  # Tell the agent it should update its parameters.\n",
    "  starttime = time()\n",
    "  play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\", nb_episodes=500, verbose=False)  # Dense rewards game.\n",
    "\n",
    "  print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
    "\n",
    "  # Save the trained agent.\n",
    "  import os\n",
    "  os.makedirs('checkpoints', exist_ok=True)\n",
    "  torch.save(agent, 'checkpoints/agent_trained_on_single_game.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdT8TB7ayE6F"
   },
   "source": [
    "#### Testing the agent trained on a single game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElRy94-tyE6F"
   },
   "outputs": [],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n",
    "agent.test()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Dense rewards game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAgebJpEyE6F"
   },
   "source": [
    "Of course, since we trained on that single simple game, it's not surprinsing the agent can achieve a high score on it. It would be more interesting to evaluate the generalization capability of the agent.\n",
    "\n",
    "To do so, we are going to test the agent on another game drawn from the same game distribution (i.e. same world but the goal is to pick another food item). Let's generate `games/another_game.z8` with the same rewards density (`--rewards dense`) and the same goal description (`--goal detailed`), but using `--seed 1` and without the `--test` flag (to make sure the game is not part of the test set since `games/rewardsDense_goalDetailed.z8` is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPKY9jkMyE6G"
   },
   "outputs": [],
   "source": [
    "!tw-make tw-simple --rewards dense --goal detailed --seed 1 --output games/tw-another_game.z8 -v -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOILdKH_yE6G"
   },
   "outputs": [],
   "source": [
    "# We report the score and steps averaged over 10 playthroughs.\n",
    "play(RandomAgent(), \"./games/tw-another_game.z8\")\n",
    "play(agent, \"./games/tw-another_game.z8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mMvSMb-yE6G"
   },
   "source": [
    "As we can see the trained agent does a bit better than the random agent. In order to improve the agent's generalization capability, we should train it on many different games drawn from the game distribution.\n",
    "\n",
    "One could use the following command to easily generate 100 training games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1xWlfjdyE6H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the data in the prequisite section.\n",
    "\n",
    "! seq 1 100 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --format z8 --output training_games/ --seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He0cUnpuyE6H"
   },
   "source": [
    "Then, we train our agent on that set of training games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVLj-YKMyE6H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the data in the prequisite section.\n",
    "if False:\n",
    "  from time import time\n",
    "  agent = NeuralAgent()\n",
    "\n",
    "  print(\"Training on 100 games\")\n",
    "  agent.train()  # Tell the agent it should update its parameters.\n",
    "  starttime = time()\n",
    "  play(agent, \"./training_games/\", nb_episodes=100 * 5, verbose=False)  # Each game will be seen 5 times.\n",
    "  print(\"Trained in {:.2f} secs\".format(time() - starttime))\n",
    "\n",
    "  # Save the trained agent.\n",
    "  import os\n",
    "  os.makedirs('checkpoints', exist_ok=True)\n",
    "  torch.save(agent, 'checkpoints/agent_trained_on_multiple_games.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_iOg8QpyE6H"
   },
   "source": [
    "#### Testing the agent trained on 100 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDn1WlTIyE6I"
   },
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n",
    "agent.test()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
    "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVVsVTyLyE6I"
   },
   "source": [
    "Compare it to the agent trained on a single game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UCCW3cvyE6I",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/agent_trained_on_single_game.pt')\n",
    "agent.test()\n",
    "play(agent, \"./games/tw-rewardsDense_goalDetailed.z8\")  # Averaged over 10 playthroughs.\n",
    "play(agent, \"./games/tw-another_game.z8\")               # Averaged over 10 playthroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B31IqhZAyE6J"
   },
   "source": [
    "#### Evaluating the agent on a test distribution\n",
    "We will generate 20 test games and evaluate the agent on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZevzwZbyE6J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can skip this if you already downloaded the games in the prequisite section.\n",
    "\n",
    "! seq 1 20 | xargs -n1 -P4 tw-make tw-simple --rewards dense --goal detailed --test --format z8 --output testing_games/ --seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHrQ8zjzyE6J"
   },
   "outputs": [],
   "source": [
    "agent = torch.load('checkpoints/agent_trained_on_multiple_games.pt')\n",
    "agent.test()\n",
    "play(RandomAgent(), \"./testing_games/\", nb_episodes=20 * 10)\n",
    "play(agent, \"./testing_games/\", nb_episodes=20 * 10)  # Averaged over 10 playthroughs for each test game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsyMnRm4yE6K"
   },
   "source": [
    "While not being perfect, the agent manage to score more points on average compared to the random agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdHfSBZ4yE6K"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Here are a few possible directions one can take to improve the agent's performance.\n",
    "- Adding more training games\n",
    "- Changing the agent architecture\n",
    "- Leveraging already trained word embeddings\n",
    "- Playing more games at once (see [`textworld.gym.make_batch`](https://textworld.readthedocs.io/en/latest/textworld.gym.html#textworld.gym.utils.make_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NepDIonbyE6K"
   },
   "source": [
    "\n",
    "## Papers about RL applied to text-based games\n",
    "* [Language Understanding for Text-based games using Deep Reinforcement Learning][narasimhan_et_al_2015]\n",
    "* [Learning How Not to Act in Text-based Games][haroush_et_al_2017]\n",
    "* [Deep Reinforcement Learning with a Natural Language Action Space][he_et_al_2015]\n",
    "* [What can you do with a rock? Affordance extraction via word embeddings][fulda_et_al_2017]\n",
    "* [Text-based adventures of the Golovin AI Agent][kostka_et_al_2017]\n",
    "* [Using reinforcement learning to learn how to play text-based games][zelinka_2018]\n",
    "\n",
    "[narasimhan_et_al_2015]: https://arxiv.org/abs/1506.08941\n",
    "[haroush_et_al_2017]: https://openreview.net/pdf?id=B1-tVX1Pz\n",
    "[he_et_al_2015]: https://arxiv.org/abs/1511.04636\n",
    "[fulda_et_al_2017]: https://arxiv.org/abs/1703.03429\n",
    "[kostka_et_al_2017]: https://arxiv.org/abs/1705.05637\n",
    "[zelinka_2018]: https://arxiv.org/abs/1801.01999"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
